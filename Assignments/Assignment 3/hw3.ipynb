{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNnHK4qKguXrcUouloJgUp8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "65549ba4b3a8498dad842e27aac00517": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5fdda5eb12c9425fac9a1ba3ec189243",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_48aedcbd300b4213bc7bfb2d21b5dc38",
              "IPY_MODEL_4b2699c6fa824ce1bc7cd4e74ce08d63",
              "IPY_MODEL_547e4be181954d8eb4ad17beab61fec0"
            ]
          }
        },
        "5fdda5eb12c9425fac9a1ba3ec189243": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "48aedcbd300b4213bc7bfb2d21b5dc38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e872ae2613f546ada237863b6e2e1f96",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_790ffd918fab4feca074e3d3c48f19cb"
          }
        },
        "4b2699c6fa824ce1bc7cd4e74ce08d63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_22030f39b6214134b2f34a4368e8b384",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 200,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 200,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0a44abc04ab644b1845d675ef0e80db5"
          }
        },
        "547e4be181954d8eb4ad17beab61fec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6cfee11fb8b2484bad395a4ac923f903",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 200/200 [00:02&lt;00:00, 69.97it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_179a836f75a04edf886c0f4b043b281d"
          }
        },
        "e872ae2613f546ada237863b6e2e1f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "790ffd918fab4feca074e3d3c48f19cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "22030f39b6214134b2f34a4368e8b384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0a44abc04ab644b1845d675ef0e80db5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6cfee11fb8b2484bad395a4ac923f903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "179a836f75a04edf886c0f4b043b281d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0b1d9714a5643008a030e2933930d47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_37dd44aa908c49dcb600052c80a89002",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4a4720afc879490d84878078856f38d2",
              "IPY_MODEL_61728e0090454433b4d68fa9472a0d3c",
              "IPY_MODEL_a8f64c73bb734bacae3bb6a45253a437"
            ]
          }
        },
        "37dd44aa908c49dcb600052c80a89002": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4a4720afc879490d84878078856f38d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_463ace480e7b4a959b07333aa84a4f2b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eaee52630bbb43a8a28ceb39730e84e9"
          }
        },
        "61728e0090454433b4d68fa9472a0d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1a84d3e4005f4d79bc3f89f1fa18347e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ebb7eacf999049a980296d3317d96a4d"
          }
        },
        "a8f64c73bb734bacae3bb6a45253a437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dc39aa13c99d4525991ff48580178b9c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10/10 [14:01&lt;00:00, 85.51s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3b981d4f8cbf42588e3a4480bc73e957"
          }
        },
        "463ace480e7b4a959b07333aa84a4f2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eaee52630bbb43a8a28ceb39730e84e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1a84d3e4005f4d79bc3f89f1fa18347e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ebb7eacf999049a980296d3317d96a4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc39aa13c99d4525991ff48580178b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3b981d4f8cbf42588e3a4480bc73e957": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pipeton8/6.864-advanced-nlp/blob/main/Assignments/Assignment%203/hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyKTDqYqxMDz"
      },
      "source": [
        "# Homework 3 - Advanced NLP\n",
        "\n",
        "(c) Felipe del Canto, October 2021\n",
        "\n",
        "_Built upon scaffolding code provided by the instructors_\n",
        "***\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N655YeL2eEUC"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!rm -rf hw3\n",
        "!wget https://github.com/pipeton8/6.864-advanced-nlp/raw/main/Assignments/Assignment%203/lab_util.py -P hw3\n",
        "!wget https://github.com/pipeton8/6.864-advanced-nlp/raw/main/Assignments/Assignment%203/reviews.csv -P hw3\n",
        "!wget https://github.com/pipeton8/6.864-advanced-nlp/raw/main/Assignments/Assignment%203/trees.zip -P hw3\n",
        "!unzip -o hw3/trees.zip -d hw3/trees\n",
        "!rm -r __MACOSX hw3/trees/__MACOSX hw3/trees.zip\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5R8vijdeKgl"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(1, '/content/hw3')\n",
        "sys.path.insert(1, '/content/hw3/trees/')\n",
        "\n",
        "import csv\n",
        "import random\n",
        "import itertools as it\n",
        "import numpy     as np\n",
        "\n",
        "import torch\n",
        "import torch.nn            as nn\n",
        "import torch.optim         as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from   torch       import cuda\n",
        "from tqdm.auto     import tqdm\n",
        "from scipy.special import logsumexp \n",
        "from span_tree     import *\n",
        "\n",
        "import lab_util"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaoYiysseNIH"
      },
      "source": [
        "# Part 1: Hidden Markov Models\n",
        "\n",
        "In Part 1 of this homework, you'll use the Baum--Welch algorithm to learn _categorical_ representations of words in your vocabulary. This uses the same dataset and lab_util as in HW 2. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePjBbOy8SZZK"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUn-q_pIeuAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8865ebb-bcdf-432f-ff52-a1c7a1df5fd6"
      },
      "source": [
        "# Set random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "data = []\n",
        "n_positive = 0\n",
        "n_disp = 0 \n",
        "with open(\"/content/hw3/reviews.csv\") as reader:\n",
        "  csvreader = csv.reader(reader)\n",
        "  next(csvreader)\n",
        "  for id, review, label in csvreader:\n",
        "    label = int(label)\n",
        "\n",
        "    # hacky class balancing\n",
        "    if label == 1:\n",
        "      if n_positive == 2000:\n",
        "        continue\n",
        "      n_positive += 1\n",
        "    if len(data) == 4000:\n",
        "      break\n",
        "\n",
        "    data.append((review, label))\n",
        "    \n",
        "    if n_disp > 5:\n",
        "      continue\n",
        "    n_disp += 1\n",
        "    print(\"review:\", review)\n",
        "    print(\"rating:\", label, \"(good)\" if label == 1 else \"(bad)\")\n",
        "    print()\n",
        "\n",
        "print(f\"Read {len(data)} total reviews.\")\n",
        "np.random.shuffle(data)\n",
        "reviews, labels = zip(*data)\n",
        "train_reviews = reviews[:3000]\n",
        "train_labels = labels[:3000]\n",
        "val_reviews = reviews[3000:3500]\n",
        "val_labels = labels[3000:3500]\n",
        "test_reviews = reviews[3500:]\n",
        "test_labels = labels[3500:]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "review: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
            "rating: 0 (bad)\n",
            "\n",
            "review: This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
            "rating: 0 (bad)\n",
            "\n",
            "review: Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.\n",
            "rating: 1 (good)\n",
            "\n",
            "Read 4000 total reviews.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgAH9ebASeBy"
      },
      "source": [
        "## Forward and Backward algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2qlqRHoe3y-"
      },
      "source": [
        "Next, implement the forward--backward algorithm for HMMs like we saw in class.\n",
        "\n",
        "**IMPORTANT NOTE**: if you directly multiply probabilities as shown on the class slides, you'll get underflow errors. You'll want to work in the log domain (remember that `log(ab) = log(a) + log(b)`, `log(exp(a) + exp(b)) = logaddexp(a, b)`). You should use the first hint whenever you want to multiply/divide numbers by adding them in log space instead, and you should use the second hint whenever you want to add numbers that are already in or could be converted to log space. In general, we recommend either `np.logaddexp` or `scipy.special.logsumexp` as safe ways to compute the necessary quantities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wVf4QVIfBdc"
      },
      "source": [
        "# HMM model\n",
        "class HMM(object):\n",
        "    def __init__(self, num_states, num_words, random_state=0):\n",
        "      # Initialize random generator using provided seed\n",
        "      rng = np.random.default_rng(random_state)\n",
        "\n",
        "      # Initialize properties\n",
        "      self.num_states = num_states\n",
        "      self.num_words = num_words\n",
        "\n",
        "      self.states = range(num_states)\n",
        "      self.symbols = range(num_words)\n",
        "\n",
        "      # Initialize matrix of transition probabilities p(j|i) from uniform\n",
        "      # distribution. The second transformation ensures the sum of rows equals 1\n",
        "      self.A = rng.uniform(size=(num_states, num_states))\n",
        "      self.A = self.A/self.A.sum(axis=1,keepdims=True)\n",
        "\n",
        "      # Initialize matrix of emission probabilities p(o|i) from uniform\n",
        "      # distribution. The second transformation ensures the sum of rows equals 1\n",
        "      self.B = rng.uniform(size=(num_states, num_words))\n",
        "      self.B = self.B/self.B.sum(axis=1,keepdims=True)\n",
        "\n",
        "      # Initialize vector of initial distribution of states from uniform\n",
        "      # distribution. The second transformation ensures the sum of pi equals 1\n",
        "      self.pi = rng.uniform(size=(num_states,))\n",
        "      self.pi = self.pi/self.pi.sum()\n",
        "\n",
        "    def generate(self, n, return_states=False, random_state=0):\n",
        "        \"\"\"This function randomly samples the HMM to generate a sequence of\n",
        "        length n.\n",
        "        \"\"\"\n",
        "\n",
        "        # Set random seed for reproducibility\n",
        "        rng = np.random.default_rng(random_state)\n",
        "\n",
        "        # Initialize the first state\n",
        "        state = rng.choice(self.states, p=self.pi)\n",
        "\n",
        "        # Initialize sequence and state lists\n",
        "        sequence = []\n",
        "        states = [state]\n",
        "        \n",
        "        # Sample\n",
        "        for i in range(n):\n",
        "            # Get the emission probs for this state\n",
        "            b = self.B[state, :]\n",
        "\n",
        "            # Emit a word\n",
        "            word = rng.choice(self.symbols, p=b)\n",
        "            sequence.append(word)\n",
        "\n",
        "            # Get the transition probs for this state\n",
        "            a = self.A[state, :]\n",
        "\n",
        "            # Update the state\n",
        "            state = rng.choice(self.states, p=a)\n",
        "            states.append(state)\n",
        "        \n",
        "        if return_states:\n",
        "          print(\"Here!\")\n",
        "          return sequence, states\n",
        "        \n",
        "        print(type(sequence))\n",
        "        return sequence\n",
        "\n",
        "    def forward(self, obs):\n",
        "        \"\"\"\n",
        "        Runs the forward algorithm. This function returns a \n",
        "        `len(obs) x  num_states` matrix where the (t, i)th entry contains \n",
        "        log p(obs[:t], hidden_state_t = i)\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize alpha matrix of forward algorithm\n",
        "        alpha = np.zeros((len(obs), self.num_states))\n",
        "        \n",
        "        # Fill first row\n",
        "        alpha[0,:] = np.log(self.pi) + np.log(self.B[:,obs[0]])\n",
        "\n",
        "        # Fill the rest of the rows\n",
        "        for t in range(1,alpha.shape[0]):\n",
        "            alpha[t,:] = np.log(self.B[:,obs[t]]) + logsumexp(alpha[t-1,:] + np.log(self.A.T), axis=1)\n",
        "            \n",
        "        return alpha\n",
        "\n",
        "    def backward(self, obs):\n",
        "        \"\"\"\n",
        "        Runs the backward algorithm. This function returns a\n",
        "        `len(obs) x num_states` matrix where the (t, i)th entry contains\n",
        "        log p(obs[t+1:] | hidden_state_t = i)\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize beta matrix of backward algorithm\n",
        "        beta = np.zeros((len(obs), self.num_states))\n",
        "\n",
        "        # Fill last row\n",
        "        beta[-1,:] = 0\n",
        "\n",
        "        # Fill the remaining rows\n",
        "        for t in [len(obs)-1-i for i in range(1,len(obs))]:\n",
        "          beta[t,:] = logsumexp(np.log(self.A) + np.log(self.B[:,obs[t+1]]) + beta[t+1,:], axis=1)\n",
        "\n",
        "        return beta\n",
        "        \n",
        "    def forward_backward(self, obs):\n",
        "        \"\"\"\n",
        "        Computes forward-backward scores\n",
        "\n",
        "        - logprob is the total log-probability of the sequence obs (marginalizing\n",
        "        over hidden states).\n",
        "\n",
        "        - gamma is a matrix of size `len(obs) x num_states`. It contains the\n",
        "        marginal probability of being in state i at time t\n",
        "\n",
        "        - xi is a tensor of size `len(obs) - 1 x num_states x num_states`. It contains\n",
        "        the marginal probability of transitioning from i to j at t.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Run forward and backward algorithms\n",
        "        alpha, beta = self.forward(obs), self.backward(obs)\n",
        "\n",
        "        # Compute logprob marginalizing p(O) over hidden states at t = len(obs)\n",
        "        logprob = logsumexp(alpha[-1,:])\n",
        "\n",
        "        # Compute gamma\n",
        "        gamma = np.exp(alpha + beta - logprob)\n",
        "\n",
        "        # Compute xi\n",
        "        xi = np.zeros((len(obs)-1, self.num_states, self.num_states))\n",
        "\n",
        "        for t in range(len(obs)-1):\n",
        "          # Compute log of xi_t\n",
        "          log_xi_t = (np.tile(alpha[t,:].reshape(-1,1), (1,self.num_states)) \n",
        "                                + np.log(self.A) \n",
        "                                + np.tile(np.log(self.B[:,obs[t+1]]).reshape(1,-1), (self.num_states,1))\n",
        "                                + np.tile(beta[t+1,:].reshape(1,-1), (self.num_states,1))\n",
        "                     ) - logprob        \n",
        "          \n",
        "          # Insert into matrix\n",
        "          xi[t, :, :] = np.exp(log_xi_t) \n",
        "\n",
        "        return logprob, xi, gamma\n",
        "\n",
        "    def learn_unsupervised(self, corpus, num_iters, verbose = True, print_every=10):\n",
        "        \"\"\"Run the Baum Welch EM algorithm\n",
        "        \n",
        "        corpus: the data to learn from\n",
        "        num_iters: the number of iterations to run the algorithm\n",
        "        print_every: how often to print the log-likelihood while the model is\n",
        "        updating its parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        for i_iter in tqdm(range(num_iters)):\n",
        "          \"\"\"\n",
        "          expected_si: a vector of size (num_states,) where the i-th entry is\n",
        "          the expected number of times a sentence is transitioning from state \n",
        "          i to some other state. Be careful about which states this includes!\n",
        "\n",
        "          expected_sij: an array of size (num_states, num_states) where the\n",
        "          (i,j)-th entry represents the expected number of state transitions\n",
        "          between state i and state j.\n",
        "\n",
        "          expected_sjwk: an array of size (num_states, num_words) where the \n",
        "          (j,k)-th entry represents the expected number of times the word w_k \n",
        "          appears when at state j.\n",
        "\n",
        "          expected_q1: a vector of size (num_states,) where the i-th entry is \n",
        "          the expected number of times state i is the first state.\n",
        "\n",
        "          total_logprob: The log of the probability of the corpus being\n",
        "          generated with the current parameters of the HMM.\n",
        "          \"\"\"\n",
        "\n",
        "          # Initialize Baum-Welch estimates\n",
        "          expected_si   = np.zeros((self.num_states,))\n",
        "          expected_sij  = np.zeros((self.num_states, self.num_states))\n",
        "          expected_sjwk = np.zeros((self.num_states, self.num_words))\n",
        "          expected_q1   = np.zeros((self.num_states,))\n",
        "          total_logprob = 0\n",
        "          \n",
        "          for review in corpus:\n",
        "            review = np.array(review)\n",
        "\n",
        "            # Run forward-backward algorithm\n",
        "            logprob, xi, gamma = self.forward_backward(review)\n",
        "\n",
        "            # Compute E(s_i -> s_*)\n",
        "            expected_si += np.sum(gamma[:-1,:], axis=0)\n",
        "\n",
        "            # Compute E(s_i -> s_j)\n",
        "            expected_sij += np.sum(xi[:-1,:,:], axis=0)\n",
        "\n",
        "            # Compute E(s_j, w_k)\n",
        "\n",
        "              ## Method 1\n",
        "            check_word_matrix = np.tile(np.arange(self.num_words).reshape(-1,1), (1,len(review)))\n",
        "            review_mask = (review.reshape(1,-1) == check_word_matrix).astype(float)\n",
        "\n",
        "            expected_sjwk += gamma.T @ review_mask.T\n",
        "            \n",
        "              ## Method 2 (deprecated)\n",
        "            # for word in range(self.num_words):\n",
        "            #   mask = (review == word).reshape(-1,1)\n",
        "\n",
        "            #   expected_sjwk[:,word] += np.sum(np.where(mask, gamma, 0), axis = 0)\n",
        "\n",
        "            # Compute E(q_1 = s_i)\n",
        "            expected_q1 += gamma[0,:]\n",
        "\n",
        "            # Compute total_logprob\n",
        "            total_logprob += logprob\n",
        "\n",
        "          if verbose and i_iter % print_every == 0:\n",
        "            print(\"log-likelihood: \", total_logprob)\n",
        "\n",
        "          # M step, recompute model parameters\n",
        "\n",
        "          A_new = np.exp(np.log(expected_sij) - np.log(expected_si[:, np.newaxis]))\n",
        "          B_new = np.exp(np.log(expected_sjwk) -  np.log(expected_sjwk.sum(axis = 1, keepdims=True)))\n",
        "          pi_new = expected_q1\n",
        "\n",
        "          # Update parameters\n",
        "          self.A = A_new/A_new.sum(axis=1, keepdims=True)\n",
        "          self.B = B_new/B_new.sum(axis=1, keepdims=True)\n",
        "          self.pi = pi_new/np.sum(pi_new)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yFF2rKuuh6U"
      },
      "source": [
        "## Test Cases\n",
        "\n",
        "The following are test cases that are meant to help you debug your code. The code involves six test suites - an initialization test, a forward test, a backward test, a forward_backward test, a baum_welch_update test, and a final end_to_end test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2wp77nzSOJ3"
      },
      "source": [
        "### Test functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3gRZ_r_vLav"
      },
      "source": [
        "def init_test():\n",
        "\n",
        "    num_states = np.random.randint(100)\n",
        "    num_words = np.random.randint(100)\n",
        "    model = HMM(num_states, num_words)\n",
        "\n",
        "    assert model.A.shape == (num_states, num_states)\n",
        "    assert model.B.shape == (num_states, num_words)\n",
        "    assert model.pi.shape == (num_states, )\n",
        "\n",
        "    assert np.linalg.norm(np.sum(model.A, axis=1) - np.ones(num_states)) < 1e-10\n",
        "    assert np.linalg.norm(np.sum(model.B, axis=1) - np.ones(num_states)) < 1e-10\n",
        "    assert np.linalg.norm(np.sum(model.pi) - 1) < 1e-10\n",
        "\n",
        "    print(\"Passed init test!\")\n",
        "\n",
        "def forward_test():\n",
        "    model = HMM(2, 10)\n",
        "    model.A = np.array([[0.79034887, 0.20965113],\n",
        "                        [0.66824331, 0.33175669]])\n",
        "    model.B = np.array([[0.08511814, 0.06627238, 0.08487461, 0.15607959, 0.00124582, 0.12984083, 0.11164849, 0.11591902, 0.15232716, 0.09667395],\n",
        "                        [0.18425462, 0.14326559, 0.14026994, 0.0215989,  0.17687124, 0.04681278, 0.05857451, 0.17451212, 0.00473382, 0.04910648]])\n",
        "    model.pi = np.array([0.77480039, 0.22519961])\n",
        "    obs = [1, 8, 0, 0, 3, 4, 5, 2, 6, 3, 7, 9]\n",
        "    alpha = model.forward(obs)\n",
        "\n",
        "    true_alpha = np.array([[-2.96913, -3.43382],\n",
        "                          [ -4.66005, -9.19418],\n",
        "                          [ -7.35001, -7.89695],\n",
        "                          [ -9.65069, -9.95363],\n",
        "                          [-11.25815, -14.27392],\n",
        "                          [-18.14079, -14.4781 ],\n",
        "                          [-16.89275, -18.62696],\n",
        "                          [-19.45549, -20.17289],\n",
        "                          [-21.53772, -23.283  ],\n",
        "                          [-23.4927, -26.69119],\n",
        "                          [-25.84891, -26.73817],\n",
        "                          [-28.12237, -29.92402]])\n",
        "\n",
        "    diff = np.linalg.norm(np.sum(np.round(alpha,5) - true_alpha, axis=1))\n",
        "\n",
        "    if diff < 1e-5:\n",
        "      print(\"Passed forward test!\")\n",
        "\n",
        "    else:\n",
        "      print(\"The result of the forward function should be\", true_alpha)\n",
        "      print(\"Your value of alpha is:\", np.round(alpha, 5))\n",
        "      print(\"diff = \", diff)\n",
        "\n",
        "def backward_test():\n",
        "    model = HMM(2, 10)\n",
        "    model.A = np.array([[0.79034887, 0.20965113],\n",
        "                        [0.66824331, 0.33175669]])\n",
        "    model.B = np.array([[0.08511814, 0.06627238, 0.08487461, 0.15607959, 0.00124582, 0.12984083, 0.11164849, 0.11591902, 0.15232716, 0.09667395],\n",
        "                        [0.18425462, 0.14326559, 0.14026994, 0.0215989,  0.17687124, 0.04681278, 0.05857451, 0.17451212, 0.00473382, 0.04910648]])\n",
        "    model.pi = np.array([0.77480039, 0.22519961])\n",
        "    obs = [1, 8, 0, 0, 3, 4, 5, 2, 6, 3, 7, 9]\n",
        "    beta = model.backward(obs)\n",
        "\n",
        "    true_beta = np.array([[-25.42937, -25.58918], \n",
        "                          [-23.32164, -23.19959],\n",
        "                          [-21.11007, -21.02033],\n",
        "                          [-18.82215, -18.94381],\n",
        "                          [-16.78523, -16.33951],\n",
        "                          [-13.42847, -13.51924],\n",
        "                          [-11.24815, -11.19161],\n",
        "                          [ -8.88679,  -8.96441],\n",
        "                          [ -6.57374,  -6.70985],\n",
        "                          [ -4.51873,  -4.47419],\n",
        "                          [ -2.44529,  -2.51463],\n",
        "                          [  0, 0]])\n",
        "\n",
        "    diff = np.linalg.norm(np.sum(np.round(beta,5) - true_beta, axis=1))\n",
        "    if diff < 1e-5:\n",
        "      print(\"Passed backward test!\")\n",
        "\n",
        "    else:\n",
        "      print(\"The result of the backward function should be\", true_beta)\n",
        "      print(\"Your value of beta is:\", np.round(beta, 5))\n",
        "      print(\"diff = \", diff)\n",
        "\n",
        "def forward_backward_test():\n",
        "    model = HMM(2, 10)\n",
        "    model.A = np.array([[0.79034887, 0.20965113],\n",
        "                        [0.66824331, 0.33175669]])\n",
        "    model.B = np.array([[0.08511814, 0.06627238, 0.08487461, 0.15607959, 0.00124582, 0.12984083, 0.11164849, 0.11591902, 0.15232716, 0.09667395],\n",
        "                        [0.18425462, 0.14326559, 0.14026994, 0.0215989,  0.17687124, 0.04681278, 0.05857451, 0.17451212, 0.00473382, 0.04910648]])\n",
        "    model.pi = np.array([0.77480039, 0.22519961])\n",
        "    obs = [1, 8, 0, 0, 3, 4, 5, 2, 6, 3, 7, 9]\n",
        "    logprob, xi, gamma = model.forward_backward(obs)\n",
        "\n",
        "    true_logprob = -27.96963\n",
        "\n",
        "    true_xi = np.array([[[0.64523, 0.00601],\n",
        "                        [0.34278, 0.00598]],\n",
        "                        [[0.60684, 0.38117],\n",
        "                        [0.00551, 0.00648]],\n",
        "                        [[0.40595, 0.2064 ],\n",
        "                        [0.19863, 0.18902]],\n",
        "                        [[0.5718,  0.03278],\n",
        "                        [0.35711, 0.03831]],\n",
        "                        [[0.02625, 0.90266],\n",
        "                        [0.00109, 0.07   ]],\n",
        "                        [[0.02482, 0.00251],\n",
        "                        [0.81777, 0.15489]],\n",
        "                        [[0.59943, 0.24316],\n",
        "                        [0.08947, 0.06793]],\n",
        "                        [[0.6143,  0.07461],\n",
        "                        [0.25347, 0.05762]],\n",
        "                        [[0.8357,  0.03207],\n",
        "                        [0.12337, 0.00886]],\n",
        "                        [[0.69872, 0.26034],\n",
        "                        [0.02412, 0.01682]],\n",
        "                        [[0.63701, 0.08583],\n",
        "                        [0.22134, 0.05582]]])\n",
        "    \n",
        "    true_gamma = np.array([[0.65124, 0.34876],\n",
        "                          [0.98802, 0.01198],\n",
        "                          [0.61235, 0.38765],\n",
        "                          [0.60458, 0.39542],\n",
        "                          [0.92891, 0.07109],\n",
        "                          [0.02733, 0.97267],\n",
        "                          [0.8426,  0.1574 ],\n",
        "                          [0.68891, 0.31109],\n",
        "                          [0.86777, 0.13223],\n",
        "                          [0.95906, 0.04094],\n",
        "                          [0.72284, 0.27716],\n",
        "                          [0.85835, 0.14165]])\n",
        "\n",
        "    diff_log = np.abs(np.exp(logprob) - np.exp(true_logprob))\n",
        "    diff_xi  = np.linalg.norm(np.round(xi,5) - true_xi)\n",
        "    diff_gamma = np.linalg.norm(np.round(gamma,5) - true_gamma)\n",
        "\n",
        "    if np.max([diff_log, diff_xi, diff_gamma]) < 1e-10:\n",
        "      print(\"Passed forward-backward test!\")\n",
        "\n",
        "    else:\n",
        "      print(\"The value of logprob should be:\", true_logprob)\n",
        "      print(\"Your value of logprob is:\", np.round(logprob, 5))\n",
        "      print(\"Diff = \", diff_log)\n",
        "      print(\"\")\n",
        "\n",
        "      print(\"The value of xi should be:\", true_xi)\n",
        "      print(\"Your value of xi is:\", np.round(xi, 5))\n",
        "      print(\"Diff = \", diff_xi)\n",
        "      print(\"\")\n",
        "\n",
        "      print(\"The value of gamma should be:\", true_gamma)\n",
        "      print(\"Your value of gamma is:\", np.round(gamma, 5))\n",
        "      print(\"Diff = \", diff_gamma)\n",
        "\n",
        "def baum_welch_update_test(num_iters=200):\n",
        "    model = HMM(4, 10)\n",
        "    \n",
        "    model.A = np.array([[0.05263151, 0.62161178, 0.06683182, 0.25892489],\n",
        "                        [0.26993274, 0.13114741, 0.32305468, 0.27586517],\n",
        "                        [0.2951958,  0.14576492, 0.22474111, 0.33429817],\n",
        "                        [0.29586018, 0.26065884, 0.1977772,  0.24570378]])\n",
        "    \n",
        "    model.B = np.array([[0.01800425, 0.09767131, 0.17824799, 0.12586453, 0.19514548, 0.05433139, 0.01995667, 0.12985343, 0.01884263, 0.16208232],\n",
        "                        [0.04512782, 0.09469685, 0.1426164,  0.13851362, 0.08717793, 0.17152532, 0.08746939, 0.04900339, 0.05315859, 0.13071069],\n",
        "                        [0.11055806, 0.10592473, 0.0051817,  0.07721441, 0.21761783, 0.20323146, 0.18881598, 0.00584989, 0.00682669, 0.07877924],\n",
        "                        [0.08711377, 0.16703645, 0.0706214,  0.05297571, 0.10486868, 0.16794587, 0.13562053, 0.15729142, 0.03345308, 0.02307309]])\n",
        "    \n",
        "    model.pi = np.array([0.21186864, 0.27156561, 0.37188523, 0.14468051])\n",
        "    \n",
        "    corpus = np.array([[7,3,2,5,0,3,2,9,4,2], [7,3,2,4,2,8,7,5,0,8], [7,3,2,3,1,7,3,8,6,7], [7,3,2,6,4,4,3,4,0,0]])\n",
        "\n",
        "    model.learn_unsupervised(corpus, num_iters)\n",
        "\n",
        "    true_A = np.array([[0, 1, 0, 0], \n",
        "                      [0.14122, 0, 0.27099, 0.58779], \n",
        "                      [0.20671, 0, 0, 0.79329], \n",
        "                      [0, 0.90909, 0.09091, 0]])\n",
        "\n",
        "    true_B = np.array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
        "                      [0.0625, 0, 0, 0.5, 0, 0.125, 0.125, 0, 0.125, 0.0625],\n",
        "                      [0, 0.20671, 0, 0, 0.79329, 0, 0, 0, 0, 0],\n",
        "                      [0.24667, 0, 0.57555, 0, 0.09556, 0, 0, 0, 0.08222, 0]])\n",
        "\n",
        "    true_pi = np.array([1, 0, 0, 0])\n",
        "\n",
        "    diff_A = np.linalg.norm(np.sum(np.round(model.A,5) - true_A, axis=1))\n",
        "    diff_B = np.linalg.norm(np.sum(np.round(model.B,5) - true_B, axis=1))\n",
        "    diff_pi = np.linalg.norm(model.pi - true_pi)\n",
        "\n",
        "    if np.max([diff_A, diff_B, diff_pi]) < 1e-10:\n",
        "      print(\"Passed Baum-Welch test!\")\n",
        "    \n",
        "    else:\n",
        "      print(\"hmm.A should be\", true_A)\n",
        "      print(\"Your implementation has hmm.A to be\", np.round(model.A, 5))\n",
        "      print(\"Diff = \", diff_A)\n",
        "      print(\"\")\n",
        "\n",
        "      print(\"hmm.B should be\", true_B)\n",
        "      print(\"Your implementation has hmm.B to be\", np.round(model.B, 5))\n",
        "      print(\"Diff = \", diff_B)\n",
        "      print(\"\")\n",
        "\n",
        "      print(\"hmm.pi should be\", true_pi)\n",
        "      print(\"Your implementation has hmm.pi to be\", np.round(model.pi, 5))\n",
        "      print(\"Diff = \", diff_pi)\n",
        "      print(\"\")\n",
        "\n",
        "def end_to_end_test():\n",
        "    # Test Case 1\n",
        "    corpus = np.array([[0,3,0,3,0,3,0,3,0,3,0,3], [0,2,0,2,0,2,0,2,0,2,0,2,0], [1,2,1,2,1,2,1,2,1,2,1,2],[1,3,1,3,1,3,1,3,1,3]])\n",
        "    hmm = HMM(num_states=2,num_words=4)\n",
        "    hmm.learn_unsupervised(corpus, 10)\n",
        "\n",
        "    true_A = np.array([[0, 1], [1, 0]])\n",
        "    true_B_1 = np.array([[0, 0, 0.5, 0.5], [0.5, 0.5, 0, 0]])\n",
        "    true_B_2 = np.array([[0.5, 0.5, 0, 0], [0, 0, 0.5, 0.5]])\n",
        "\n",
        "    diff_A = np.linalg.norm(np.round(hmm.A,5) - true_A)\n",
        "    diff_B_1 = np.linalg.norm(np.round(hmm.B,5) - true_B_1)\n",
        "    diff_B_2 = np.linalg.norm(np.round(hmm.B,5) - true_B_2)\n",
        "\n",
        "    if diff_A < 1e-10 and (diff_B_1 < 1e-10 or diff_B_2 < 1e-10):\n",
        "      print(\"End-to-end test #1 passed!\")\n",
        "    \n",
        "    else:\n",
        "      print(\"After this test case, hmm.A should be approximately,\", true_A)\n",
        "      print(\"This is your current value of hmm.A: \", np.round(hmm.A, 5))\n",
        "      print(\"Diff = \", diff_A)\n",
        "      print(\"\")\n",
        "\n",
        "      print(\"After this test case, hmm.B should either be approximately,\", true_B_1 , \" or it should be \", true_B_2)\n",
        "      print(\"This is your current value of hmm.B: \", np.round(hmm.B, 5))\n",
        "      print(\"Diff option 1 = \", diff_B_1)\n",
        "      print(\"Diff option 2 = \", diff_B_2)\n",
        "      print(\"\")\n",
        "\n",
        "    # Test Case 2\n",
        "    corpus = np.array([[0,0,0,0,0,0,0,0,0,0], [1,1,1,1,1,1,1,1,1,1], [2,2,2,2,2,2,2,2,2,2]])\n",
        "    hmm = HMM(num_states=3, num_words=3)\n",
        "    hmm.learn_unsupervised(corpus, 100)\n",
        "\n",
        "    true_A = np.eye(3)\n",
        "\n",
        "    diff_A = np.linalg.norm(np.round(hmm.A,5) - true_A)\n",
        "\n",
        "    if diff_A < 1e-10:\n",
        "      print(\"End-to-end test #1 passed for matrix A!\")\n",
        "\n",
        "    else:\n",
        "      print(\"After this test case, hmm.A should be approximately,\", true_A)\n",
        "      print(\"This is your current value of hmm.A: \", np.round(hmm.A, 5))\n",
        "      print(\"Diff = \", diff_A)\n",
        "      print(\"\")\n",
        "    \n",
        "    print(\"After this test case, hmm.B should be some 3 by 3 permutation matrix\")\n",
        "    print(\"This is your current value of hmm.B: \", np.round(hmm.B, 5))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmewPFV2MPlS"
      },
      "source": [
        "### Run tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFtzZ9W9MVKu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "65549ba4b3a8498dad842e27aac00517",
            "5fdda5eb12c9425fac9a1ba3ec189243",
            "48aedcbd300b4213bc7bfb2d21b5dc38",
            "4b2699c6fa824ce1bc7cd4e74ce08d63",
            "547e4be181954d8eb4ad17beab61fec0",
            "e872ae2613f546ada237863b6e2e1f96",
            "790ffd918fab4feca074e3d3c48f19cb",
            "22030f39b6214134b2f34a4368e8b384",
            "0a44abc04ab644b1845d675ef0e80db5",
            "6cfee11fb8b2484bad395a4ac923f903",
            "179a836f75a04edf886c0f4b043b281d"
          ]
        },
        "outputId": "1ef347ab-e928-4058-b5ce-54379179240a"
      },
      "source": [
        "init_test()\n",
        "forward_test()\n",
        "backward_test()\n",
        "forward_backward_test()\n",
        "baum_welch_update_test()\n",
        "# end_to_end_test()\n",
        "\n",
        "# \"\"\"\n",
        "# Note: The end_to_end_test is not as robust due to it using random starts. Try\n",
        "# running the test case a few times to see if you get a good result at least a few\n",
        "# times before deciding that your code is buggy.\n",
        "# \"\"\""
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed init test!\n",
            "Passed forward test!\n",
            "Passed backward test!\n",
            "Passed forward-backward test!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65549ba4b3a8498dad842e27aac00517",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/200 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log-likelihood:  -96.33989919755487\n",
            "log-likelihood:  -61.78830245967527\n",
            "log-likelihood:  -60.1638600571547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:76: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:99: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:135: RuntimeWarning: divide by zero encountered in log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log-likelihood:  -60.09338243702382\n",
            "log-likelihood:  -60.094770227097676\n",
            "log-likelihood:  -60.09480415290434\n",
            "log-likelihood:  -60.09480501495133\n",
            "log-likelihood:  -60.09480503960428\n",
            "log-likelihood:  -60.09480504033906\n",
            "log-likelihood:  -60.09480504036118\n",
            "log-likelihood:  -60.09480504036185\n",
            "log-likelihood:  -60.094805040361884\n",
            "log-likelihood:  -60.09480504036187\n",
            "log-likelihood:  -60.09480504036188\n",
            "log-likelihood:  -60.09480504036188\n",
            "log-likelihood:  -60.09480504036187\n",
            "log-likelihood:  -60.09480504036187\n",
            "log-likelihood:  -60.09480504036187\n",
            "log-likelihood:  -60.09480504036187\n",
            "log-likelihood:  -60.09480504036186\n",
            "hmm.A should be [[0.      1.      0.      0.     ]\n",
            " [0.14122 0.      0.27099 0.58779]\n",
            " [0.20671 0.      0.      0.79329]\n",
            " [0.      0.90909 0.09091 0.     ]]\n",
            "Your implementation has hmm.A to be [[0.      1.      0.      0.     ]\n",
            " [0.      0.      0.34332 0.65668]\n",
            " [0.21765 0.33615 0.4462  0.     ]\n",
            " [0.      1.      0.      0.     ]]\n",
            "Diff =  4.163336342344337e-17\n",
            "\n",
            "hmm.B should be [[0.      0.      0.      0.      0.      0.      0.      1.      0.\n",
            "  0.     ]\n",
            " [0.0625  0.      0.      0.5     0.      0.125   0.125   0.      0.125\n",
            "  0.0625 ]\n",
            " [0.      0.20671 0.      0.      0.79329 0.      0.      0.      0.\n",
            "  0.     ]\n",
            " [0.24667 0.      0.57555 0.      0.09556 0.      0.      0.      0.08222\n",
            "  0.     ]]\n",
            "Your implementation has hmm.B to be [[0.      0.      0.      0.      0.      0.      0.      1.      0.\n",
            "  0.     ]\n",
            " [0.      0.      0.      0.50078 0.06103 0.1252  0.1252  0.      0.1252\n",
            "  0.0626 ]\n",
            " [0.28331 0.11302 0.14876 0.      0.45491 0.      0.      0.      0.\n",
            "  0.     ]\n",
            " [0.14673 0.      0.55849 0.      0.      0.      0.      0.19652 0.09826\n",
            "  0.     ]]\n",
            "Diff =  1.0000000000023879e-05\n",
            "\n",
            "hmm.pi should be [1 0 0 0]\n",
            "Your implementation has hmm.pi to be [1. 0. 0. 0.]\n",
            "Diff =  1.2969150403720534e-153\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF-l7WucpCBP"
      },
      "source": [
        "## Training\n",
        "\n",
        "Train a model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTWXUt15pDg4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "e0b1d9714a5643008a030e2933930d47",
            "37dd44aa908c49dcb600052c80a89002",
            "4a4720afc879490d84878078856f38d2",
            "61728e0090454433b4d68fa9472a0d3c",
            "a8f64c73bb734bacae3bb6a45253a437",
            "463ace480e7b4a959b07333aa84a4f2b",
            "eaee52630bbb43a8a28ceb39730e84e9",
            "1a84d3e4005f4d79bc3f89f1fa18347e",
            "ebb7eacf999049a980296d3317d96a4d",
            "dc39aa13c99d4525991ff48580178b9c",
            "3b981d4f8cbf42588e3a4480bc73e957"
          ]
        },
        "outputId": "250300cc-47a8-4bdf-87e6-8a9ac42cb7bf"
      },
      "source": [
        "tokenizer = lab_util.Tokenizer()\n",
        "tokenizer.fit(train_reviews)\n",
        "train_reviews_tk = tokenizer.tokenize(train_reviews)\n",
        "print(tokenizer.vocab_size)\n",
        "\n",
        "hmm = HMM(num_states=10, num_words=tokenizer.vocab_size)\n",
        "hmm.learn_unsupervised(train_reviews_tk, 10)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2006\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0b1d9714a5643008a030e2933930d47",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log-likelihood:  -2064727.2595864385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiCwE05xqXmI"
      },
      "source": [
        "Let's look at some of the words associated with each hidden state:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXhMoLUFqbn_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1f92f2-760a-49d4-def5-0167fcb69c94"
      },
      "source": [
        "for i in range(hmm.num_states):\n",
        "    most_probable = np.argsort(hmm.B[i, :])[-10:][::-1]\n",
        "    print(f\"state {i}\")\n",
        "    for o in most_probable:\n",
        "        print(tokenizer.token_to_word[o], hmm.B[i, o])\n",
        "    print()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state 0\n",
            "the 0.08432173266152884\n",
            "a 0.07178421576630233\n",
            "<unk> 0.06293330586725004\n",
            ", 0.052707339198705336\n",
            ". 0.03738488407635153\n",
            "of 0.029711783191867007\n",
            "my 0.02759277411877342\n",
            "this 0.02147933838934088\n",
            "and 0.018193706104061614\n",
            "to 0.016017528160705702\n",
            "\n",
            "state 1\n",
            "<unk> 0.06682721089083359\n",
            ", 0.06300753977730857\n",
            ". 0.033462740322967394\n",
            "of 0.025904187398084838\n",
            "for 0.02222486045358867\n",
            "the 0.02181833507037758\n",
            "in 0.018321260202549643\n",
            "and 0.01388430608018548\n",
            "to 0.013831480302573992\n",
            "but 0.012821968688751538\n",
            "\n",
            "state 2\n",
            "i 0.1933391605508423\n",
            ". 0.08717876540394771\n",
            ", 0.053602169864646915\n",
            "this 0.04895082674709242\n",
            "and 0.0347004004119911\n",
            "<unk> 0.0345407673587977\n",
            "it 0.03428330761095251\n",
            "br 0.033073632527272044\n",
            "to 0.01882175057791427\n",
            "of 0.017379382058426186\n",
            "\n",
            "state 3\n",
            ". 0.05352483453937355\n",
            "the 0.04220223072613798\n",
            "it 0.03708302902713894\n",
            "br 0.035483932688426055\n",
            "<unk> 0.03073428518358623\n",
            "a 0.02471390246952807\n",
            "this 0.022329013484913438\n",
            "are 0.018892176829008196\n",
            "they 0.01771955788953885\n",
            "of 0.015823206515854185\n",
            "\n",
            "state 4\n",
            "<unk> 0.12147644944543991\n",
            "the 0.07617029179482479\n",
            ", 0.05381248031627944\n",
            "and 0.04548102815729984\n",
            ". 0.036558872198492035\n",
            "a 0.03311767995455095\n",
            "to 0.028971786551929935\n",
            "of 0.019620959164756365\n",
            "is 0.018760089300253125\n",
            "in 0.011786437844950235\n",
            "\n",
            "state 5\n",
            "<unk> 0.1516634032542005\n",
            ". 0.12098165615419255\n",
            "and 0.0275218114279445\n",
            "but 0.023054716062744184\n",
            ", 0.018808218999651793\n",
            "to 0.018343971441910408\n",
            "the 0.012961095069510134\n",
            "! 0.01046215201495149\n",
            "that 0.010288947886136752\n",
            "br 0.009047787570047337\n",
            "\n",
            "state 6\n",
            "the 0.07518630635710834\n",
            "<unk> 0.065308113414575\n",
            ". 0.05535087074888906\n",
            "it 0.03715690447522628\n",
            "and 0.03309201694494557\n",
            "br 0.026801379609206293\n",
            "is 0.02311885864280033\n",
            "i 0.02195876518855513\n",
            "they 0.019306382574550644\n",
            ", 0.018598865336048123\n",
            "\n",
            "state 7\n",
            ". 0.09307840679955358\n",
            "<unk> 0.07647376311060929\n",
            "and 0.034485896368949005\n",
            "to 0.031030207373113856\n",
            ", 0.02955262791558547\n",
            "i 0.027317821578103217\n",
            "it 0.02208824331072947\n",
            "for 0.019063402379432666\n",
            "br 0.0179127339041341\n",
            "of 0.015173661721558356\n",
            "\n",
            "state 8\n",
            ". 0.05287832925783811\n",
            "is 0.03843088075546681\n",
            "was 0.03306813618404568\n",
            "br 0.023583865978461183\n",
            "have 0.021972357951805688\n",
            "in 0.019693939572911143\n",
            "would 0.01741828590874002\n",
            "the 0.016094858400727347\n",
            "love 0.014977811055611375\n",
            "are 0.014467741502327281\n",
            "\n",
            "state 9\n",
            "<unk> 0.12410276956830481\n",
            "a 0.04240340474349064\n",
            ", 0.03252493551936522\n",
            "i 0.03092838141208874\n",
            ". 0.02999750623119386\n",
            "to 0.026786948539036362\n",
            "it 0.015015046106700515\n",
            "is 0.014866111818724414\n",
            "in 0.013927534006056943\n",
            "have 0.012652196301129237\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAQ_PmASwdFz"
      },
      "source": [
        "We can also look at some samples from the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj1eT3s3wgFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f0a2c1-2f9e-4835-fdd1-562dcf55feb9"
      },
      "source": [
        "for i in range(10):\n",
        "  print(tokenizer.de_tokenize([hmm.generate(10, random_state=i)]))"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "['to <unk> read wife thought <unk> <unk> . does i']\n",
            "<class 'list'>\n",
            "['label takes or full <unk> is oz by the much']\n",
            "<class 'list'>\n",
            "['i they juice <unk> chips <unk> is them times and']\n",
            "<class 'list'>\n",
            "['and was taste not . or chips compared would to']\n",
            "<class 'list'>\n",
            "['use <unk> to . ? my second i source 10']\n",
            "<class 'list'>\n",
            "[\"because the the <unk> gummy in meat mix don't <unk>\"]\n",
            "<class 'list'>\n",
            "['i to try the . 100 crumbs mix with also']\n",
            "<class 'list'>\n",
            "['without for 8 greasy it like were quality end tired']\n",
            "<class 'list'>\n",
            "['picture right good the for and , br br chips']\n",
            "<class 'list'>\n",
            "['like well mouth sodium of <unk> too mix have our']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9Qk9adNr7lQ"
      },
      "source": [
        "Finally, let's repeat the classification experiment from HW 2, using the _vector of expected hidden state counts_ as a sentence representation.\n",
        "\n",
        "(Warning! results may not be the same as in earlier versions of this experiment.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL6JQXLJspyA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff4d32d0-92a5-4ed3-b348-f742fea4c4ac"
      },
      "source": [
        "def train_model(xs_featurized, ys):\n",
        "  import sklearn.linear_model\n",
        "  model = sklearn.linear_model.LogisticRegression()\n",
        "  model.fit(xs_featurized, ys)\n",
        "  return model\n",
        "\n",
        "def eval_model(model, xs_featurized, ys):\n",
        "  pred_ys = model.predict(xs_featurized)\n",
        "  print(\"test accuracy\", np.mean(pred_ys == ys))\n",
        "\n",
        "def training_experiment(name, featurizer, n_train):\n",
        "    print(f\"{name} features, {n_train} examples\")\n",
        "    train_xs = np.array([\n",
        "        hmm_featurizer(review) \n",
        "        for review in tokenizer.tokenize(train_reviews[:n_train])\n",
        "    ])\n",
        "    train_ys = train_labels[:n_train]\n",
        "    test_xs = np.array([\n",
        "        hmm_featurizer(review)\n",
        "        for review in tokenizer.tokenize(test_reviews)\n",
        "    ])\n",
        "    test_ys = test_labels\n",
        "    model = train_model(train_xs, train_ys)\n",
        "    eval_model(model, test_xs, test_ys)\n",
        "    print()\n",
        "\n",
        "def hmm_featurizer(review):\n",
        "    _, _, gamma = hmm.forward_backward(review)\n",
        "    return gamma.sum(axis=0)\n",
        "\n",
        "training_experiment(\"hmm\", hmm_featurizer, n_train=100)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hmm features, 100 examples\n",
            "test accuracy 0.52\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6DI4otm0YHe"
      },
      "source": [
        "## Experiments for Part 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovbrCIUT0NGy"
      },
      "source": [
        "# Your code here!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb57m6BFySpz"
      },
      "source": [
        "# Part 2: Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY7OK7jKHN4C"
      },
      "source": [
        "## **Introduction**\n",
        "Welcome to **Homework 3 - Trees**! In this part of the homework, we're moving on from HMMs (Part 1) to using trees. We will practice *parsing* on sentences from a semantic parsing corpus.  \n",
        "\n",
        "The data is obtained from this [paper](https://arxiv.org/pdf/1810.07942.pdf) (see Figure 1). As you can see from the figure, the purpose of this task is to understand what are the users *intents* from a query in plain text.  \n",
        "\n",
        "The end goal is that given sentence to decode a binary **tree structure** with *semantic tags* as *nodes*. For example:\n",
        "\n",
        "> whats there to do this weekend -> [<font color='00b8d4'>IN:GET_EVENT</font> whats there to do [<font color='00b8d4'>SL:DATE_TIME</font> this weekend]]  \n",
        "\n",
        "Note that the brackets [<font color='00b8d4'>LABEL</font> a substring of the text] indicates that this span is a sub-tree and <font color='00b8d4'>LABEL</font>  is the semantic label of the root of the sub-tree. You might read more about bracket representation in this [tutorial](https://www.tutorialspoint.com/binary-tree-to-string-with-brackets-in-cplusplus). \n",
        "\n",
        "1. In **Part A**, we formulate this problem as a simple classification problem --- the input to the classifier will be `(text, span)` and the output will be the semantic `label` of that span. `span`  is represented by two integer `(i,j)` which are the start and the end locations of the span.\n",
        "\n",
        "2. In **Part B**, we will implement a **CKY**-style decoding algorithm to decode the final tree based on the classifier we trained in Part A.\n",
        "\n",
        "We did pre-processing to enable CKY-style decoding for you. This includes binarization of the trees and handling of unary rules. (see the [code](https://github.mit.edu/tianxing/mit_6864_hw3_202003)).  \n",
        "\n",
        "Let's start by loading some dependencies and downloading the data as usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIOkqXIoHN4H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b701122-343e-40bf-b655-07e89389936f"
      },
      "source": [
        "seed = 0\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "if cuda.is_available():\n",
        "  device = 'cuda'\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "else:\n",
        "  print('WARNING: you are running this assignment on a cpu!')\n",
        "  device = 'cpu'"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: you are running this assignment on a cpu!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mQzMXTeHN4M"
      },
      "source": [
        "## **Agenda**\n",
        "\n",
        "We apply a model that learns the parsing structures in 4 steps.\n",
        "\n",
        "1. Enumerating all possible spans of a sentence\n",
        "2. Generating word and span embeddings\n",
        "3. Learning span label classifications\n",
        "4. Decoding a tree structure using the classification distributions of spans\n",
        "\n",
        "We go through this process step by step through the homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnUOkaRgqRbW"
      },
      "source": [
        "## **PART A**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvYHEHBJwuk4"
      },
      "source": [
        "### **Data Processing**\n",
        "\n",
        "The very first step of the project is to load the corpus, building the **vocabulary**, **span label set**, and **span indices**. \n",
        "\n",
        "We first need to enumerate every node of a tree with a Depth First Search (DFS)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAauBjH1d9Up"
      },
      "source": [
        "def tree_dfs(node, span_list, label_dict, mode):\n",
        "    \"\"\"\n",
        "    The base function for the recursion:\n",
        "      node: current root while traversing the tree\n",
        "      span_list: keep tracks of the spans and their label encodings in the tree e.g [[(0,1), 1], [(0,6),45] ...] \n",
        "      label_dict: mapping from label to their encodings e.g {\"UNK\":0, \"Token\":1,\"None\":2, ... }\n",
        "      mode: \"train\" or \"eval\"\n",
        "    \"\"\"\n",
        "  \n",
        "    if len(node.children) == 0:\n",
        "        assert(type(node) == Token)\n",
        "        cur_span = (node.index, node.index + 1)\n",
        "        cur_label = label_dict['Token']\n",
        "        span_list.append([cur_span, cur_label])\n",
        "        return span_list, label_dict\n",
        "        \n",
        "    cur_span = node.get_token_span()\n",
        "    cur_label = node.label\n",
        "\n",
        "    if node.label in label_dict:\n",
        "        cur_label = label_dict[node.label]\n",
        "    \n",
        "    elif mode == 'train': # we are constructing the label dictionary\n",
        "        cur_label = len(label_dict)\n",
        "        label_dict[node.label] = cur_label\n",
        "    \n",
        "    else:\n",
        "        cur_label = label_dict['UNK']\n",
        "    \n",
        "    span_list.append([cur_span, cur_label])\n",
        "    \n",
        "    # If only has one child, we will ignore the Token label, otherwise the token \n",
        "    # span would have two conflicting labels\n",
        "    if len(node.children) > 1: \n",
        "        for child in node.children:\n",
        "            span_list, label_dict = tree_dfs(child, span_list, label_dict, mode) \n",
        "\n",
        "    return span_list, label_dict"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42LFAdiqeCCE"
      },
      "source": [
        "Now, we go through the corpus and construct the **vocab dictionary** and the **label dictionary**. Note that we just add new words and labels to the dictionaries while building the training set. Unseen words or labels in validation and test set are marked as unknown (UNK)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHZ4qUgoHN4N"
      },
      "source": [
        "def process_line(line, vocab_dict, label_dict, mode):\n",
        "    '''\n",
        "    Processing a line in the corpus.\n",
        "    line format: Sentence \\t Sentence_Tree \\n\n",
        "    \n",
        "    Example:\n",
        "        'what is the shortest way home\\t\n",
        "        [IN:GET_DIRECTIONS what [SUB is [SUB the [SUB shortest [SUB way [SL:DESTINATION home ] ] ] ] ] ]\\n'\n",
        "    \n",
        "    Inputs:\n",
        "    vocab_dict: vocab dictionary {word: word_index, ...}\n",
        "    labels_dict: label dictionary {label: label_index, ...}\n",
        "    mode: \"train\" or \"eval\"\n",
        "    '''\n",
        "    s, s_tree = line.strip().split('\\t')\n",
        "    words = s.split(' ')\n",
        "    word_ids = []\n",
        "    for word in words:\n",
        "        if word in vocab_dict:\n",
        "            word_ids.append(vocab_dict[word])\n",
        "        elif mode == 'train':\n",
        "            word_ids.append(len(vocab_dict))\n",
        "            vocab_dict[word] = len(vocab_dict)\n",
        "        else:\n",
        "            word_ids.append(vocab_dict['UNK'])\n",
        "    \n",
        "    tree = Tree(s_tree)\n",
        "    span_list = []\n",
        "    span_list, label_dict = tree_dfs(tree.root.children[0], span_list, label_dict, mode)\n",
        "    return word_ids, span_list, vocab_dict, label_dict\n",
        "\n",
        "def process_corpus(corpus_path, mode, vocab_dict=None, label_dict=None):\n",
        "    lines = open(corpus_path).readlines()\n",
        "    if not vocab_dict:\n",
        "        vocab_dict = {'UNK': 0}\n",
        "    if not label_dict:\n",
        "        label_dict = {'UNK': 0, 'Token': 1, 'None': 2}\n",
        "    corpus = []\n",
        "    sent_spans = []\n",
        "    raw_lines = []\n",
        "    for line in lines:\n",
        "      if len(line.strip()) < 3: \n",
        "        continue\n",
        "      word_ids, span_list, vocab_dict, label_dict = process_line(line, vocab_dict, label_dict, mode)\n",
        "      corpus.append(word_ids)\n",
        "      sent_spans.append(span_list)\n",
        "      raw_lines.append(line)\n",
        "    return corpus, sent_spans, vocab_dict, label_dict, raw_lines\n",
        "\n"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbhhKRL5246g"
      },
      "source": [
        "corpus_train, spans_train, vocab_dict, label_dict, train_lines = process_corpus('/content/hw3/trees/train.txt', 'train')\n",
        "corpus_valid, spans_valid, _, _, valid_lines = process_corpus('/content/hw3/trees/valid.txt', 'eval',\n",
        "                                                 vocab_dict=vocab_dict, label_dict=label_dict)\n",
        "corpus_test,  spans_test, _, _, test_lines = process_corpus('/content/hw3/trees/test.txt', 'eval',\n",
        "                                                 vocab_dict=vocab_dict, label_dict=label_dict)"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHXGTFnPm3yV"
      },
      "source": [
        "# inverted dictionaries {word_index: word, ...}\n",
        "inv_vocab_dict = np.array(list(vocab_dict.keys()))\n",
        "inv_label_dict = np.array(list(label_dict.keys()))"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RZxYTjym00m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ff6215-75bb-419b-c701-3092df74ddcc"
      },
      "source": [
        "num_words = len(vocab_dict)\n",
        "num_labels = len(label_dict)\n",
        "\n",
        "print('Number of different words: {}'.format(num_words))\n",
        "print('Number of different labels: {}'.format(num_labels))"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of different words: 8626\n",
            "Number of different labels: 147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGc5tgQtfunf"
      },
      "source": [
        "Let see how the data looks like, and compare with our output in below:\n",
        "```\n",
        "['how', 'long', 'will', 'it', 'take', 'to', 'drive', 'from', 'chicago', 'to', 'mississippi']\n",
        "how long will it take to drive from chicago to mississippi\t[IN:GET_ESTIMATED_DURATION how [SUB long [SUB will [SUB it [SUB take [SUB to [SUB [SL:METHOD_TRAVEL drive ] [SUB from [SUB [SL:SOURCE chicago ] [SUB to [SL:DESTINATION mississippi ] ] ] ] ] ] ] ] ] ] ]\n",
        "\n",
        "[[(0, 11), 3], [(0, 1), 1], [(1, 11), 4], [(1, 2), 1], [(2, 11), 4], [(2, 3), 1], [(3, 11), 4], [(3, 4), 1], [(4, 11), 4], [(4, 5), 1], [(5, 11), 4], [(5, 6), 1], [(6, 11), 4], [(6, 7), 5], [(7, 11), 4], [(7, 8), 1], [(8, 11), 4], [(8, 9), 6], [(9, 11), 4], [(9, 10), 1], [(10, 11), 7]]\n",
        "['will', 'it', 'take', 'shorter', 'to', 'get', 'to', 'the', 'white', 'house', 'by', 'bus', 'or', 'taxi', '?']\n",
        "will it take shorter to get to the white house by bus or taxi ?\t[IN:UNSUPPORTED_NAVIGATION will [SUB it [SUB take [SUB shorter [SUB to [SUB get [SUB to [SUB the [SUB white [SUB house [SUB by [SUB bus [SUB or [SUB taxi ? ] ] ] ] ] ] ] ] ] ] ] ] ] ]\n",
        "\n",
        "[[(0, 15), 8], [(0, 1), 1], [(1, 15), 4], [(1, 2), 1], [(2, 15), 4], [(2, 3), 1], [(3, 15), 4], [(3, 4), 1], [(4, 15), 4], [(4, 5), 1], [(5, 15), 4], [(5, 6), 1], [(6, 15), 4], [(6, 7), 1], [(7, 15), 4], [(7, 8), 1], [(8, 15), 4], [(8, 9), 1], [(9, 15), 4], [(9, 10), 1], [(10, 15), 4], [(10, 11), 1], [(11, 15), 4], [(11, 12), 1], [(12, 15), 4], [(12, 13), 1], [(13, 15), 4], [(13, 14), 1], [(14, 15), 1]]\n",
        "['will', 'i', 'make', 'it', 'to', 'the', 'beach', 'by', 'noon', 'if', 'i', 'leave', 'now']\n",
        "will i make it to the beach by noon if i leave now\t[IN:GET_ESTIMATED_ARRIVAL will [SUB i [SUB make [SUB it [SUB to [SUB [SL:DESTINATION--IN:GET_LOCATION--SL:CATEGORY_LOCATION the beach ] [SUB [SL:DATE_TIME_ARRIVAL by noon ] [SUB if [SUB i [SUB leave [SL:DATE_TIME_DEPARTURE now ] ] ] ] ] ] ] ] ] ] ]\n",
        "\n",
        "[[(0, 13), 9], [(0, 1), 1], [(1, 13), 4], [(1, 2), 1], [(2, 13), 4], [(2, 3), 1], [(3, 13), 4], [(3, 4), 1], [(4, 13), 4], [(4, 5), 1], [(5, 13), 4], [(5, 7), 10], [(5, 6), 1], [(6, 7), 1], [(7, 13), 4], [(7, 9), 11], [(7, 8), 1], [(8, 9), 1], [(9, 13), 4], [(9, 10), 1], [(10, 13), 4], [(10, 11), 1], [(11, 13), 4], [(11, 12), 1], [(12, 13), 12]]\n",
        "['when', 'should', 'i', 'leave', 'my', 'house', 'to', 'get', 'to', 'the', 'hamilton', 'mall', 'right', 'when', 'it', 'opens', 'on', 'saturday']\n",
        "when should i leave my house to get to the hamilton mall right when it opens on saturday\t[IN:GET_ESTIMATED_DEPARTURE when [SUB should [SUB i [SUB leave [SUB [SL:SOURCE--IN:GET_LOCATION_HOME [SL:CONTACT my ] house ] [SUB to [SUB get [SUB to [SUB [SL:DESTINATION--IN:GET_LOCATION--SL:POINT_ON_MAP the [SUB hamilton mall ] ] [SL:DATE_TIME_ARRIVAL right [SUB when [SUB it [SUB opens [SUB on saturday ] ] ] ] ] ] ] ] ] ] ] ] ] ]\n",
        "\n",
        "[[(0, 18), 13], [(0, 1), 1], [(1, 18), 4], [(1, 2), 1], [(2, 18), 4], [(2, 3), 1], [(3, 18), 4], [(3, 4), 1], [(4, 18), 4], [(4, 6), 14], [(4, 5), 15], [(5, 6), 1], [(6, 18), 4], [(6, 7), 1], [(7, 18), 4], [(7, 8), 1], [(8, 18), 4], [(8, 9), 1], [(9, 18), 4], [(9, 12), 16], [(9, 10), 1], [(10, 12), 4], [(10, 11), 1], [(11, 12), 1], [(12, 18), 11], [(12, 13), 1], [(13, 18), 4], [(13, 14), 1], [(14, 18), 4], [(14, 15), 1], [(15, 18), 4], [(15, 16), 1], [(16, 18), 4], [(16, 17), 1], [(17, 18), 1]]\n",
        "['i', 'need', 'to', 'know', 'if', 'there', \"'s\", 'a', 'lot', 'of', 'traffic', 'on', 'my', 'way', 'home']\n",
        "i need to know if there 's a lot of traffic on my way home\t[IN:GET_INFO_TRAFFIC i [SUB need [SUB to [SUB know [SUB if [SUB there [SUB 's [SUB a [SUB lot [SUB of [SUB traffic [SUB on [SUB my [SUB way [SL:DESTINATION--IN:GET_LOCATION_HOME home ] ] ] ] ] ] ] ] ] ] ] ] ] ] ]\n",
        "\n",
        "[[(0, 15), 17], [(0, 1), 1], [(1, 15), 4], [(1, 2), 1], [(2, 15), 4], [(2, 3), 1], [(3, 15), 4], [(3, 4), 1], [(4, 15), 4], [(4, 5), 1], [(5, 15), 4], [(5, 6), 1], [(6, 15), 4], [(6, 7), 1], [(7, 15), 4], [(7, 8), 1], [(8, 15), 4], [(8, 9), 1], [(9, 15), 4], [(9, 10), 1], [(10, 15), 4], [(10, 11), 1], [(11, 15), 4], [(11, 12), 1], [(12, 15), 4], [(12, 13), 1], [(13, 15), 4], [(13, 14), 1], [(14, 15), 18]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTXUkgqxe3Rw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b52edb5-0f54-4316-955a-73626c8a1f41"
      },
      "source": [
        "for i in range(5):\n",
        "  print([inv_vocab_dict[w] for w in corpus_train[i]])\n",
        "  print(train_lines[i], end=\"\")\n",
        "  print(spans_train[i])\n",
        "  print()"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['how', 'long', 'will', 'it', 'take', 'to', 'drive', 'from', 'chicago', 'to', 'mississippi']\n",
            "how long will it take to drive from chicago to mississippi\t[IN:GET_ESTIMATED_DURATION how [SUB long [SUB will [SUB it [SUB take [SUB to [SUB [SL:METHOD_TRAVEL drive ] [SUB from [SUB [SL:SOURCE chicago ] [SUB to [SL:DESTINATION mississippi ] ] ] ] ] ] ] ] ] ] ]\n",
            "[[(0, 11), 3], [(0, 1), 1], [(1, 11), 4], [(1, 2), 1], [(2, 11), 4], [(2, 3), 1], [(3, 11), 4], [(3, 4), 1], [(4, 11), 4], [(4, 5), 1], [(5, 11), 4], [(5, 6), 1], [(6, 11), 4], [(6, 7), 5], [(7, 11), 4], [(7, 8), 1], [(8, 11), 4], [(8, 9), 6], [(9, 11), 4], [(9, 10), 1], [(10, 11), 7]]\n",
            "\n",
            "['will', 'it', 'take', 'shorter', 'to', 'get', 'to', 'the', 'white', 'house', 'by', 'bus', 'or', 'taxi', '?']\n",
            "will it take shorter to get to the white house by bus or taxi ?\t[IN:UNSUPPORTED_NAVIGATION will [SUB it [SUB take [SUB shorter [SUB to [SUB get [SUB to [SUB the [SUB white [SUB house [SUB by [SUB bus [SUB or [SUB taxi ? ] ] ] ] ] ] ] ] ] ] ] ] ] ]\n",
            "[[(0, 15), 8], [(0, 1), 1], [(1, 15), 4], [(1, 2), 1], [(2, 15), 4], [(2, 3), 1], [(3, 15), 4], [(3, 4), 1], [(4, 15), 4], [(4, 5), 1], [(5, 15), 4], [(5, 6), 1], [(6, 15), 4], [(6, 7), 1], [(7, 15), 4], [(7, 8), 1], [(8, 15), 4], [(8, 9), 1], [(9, 15), 4], [(9, 10), 1], [(10, 15), 4], [(10, 11), 1], [(11, 15), 4], [(11, 12), 1], [(12, 15), 4], [(12, 13), 1], [(13, 15), 4], [(13, 14), 1], [(14, 15), 1]]\n",
            "\n",
            "['will', 'i', 'make', 'it', 'to', 'the', 'beach', 'by', 'noon', 'if', 'i', 'leave', 'now']\n",
            "will i make it to the beach by noon if i leave now\t[IN:GET_ESTIMATED_ARRIVAL will [SUB i [SUB make [SUB it [SUB to [SUB [SL:DESTINATION--IN:GET_LOCATION--SL:CATEGORY_LOCATION the beach ] [SUB [SL:DATE_TIME_ARRIVAL by noon ] [SUB if [SUB i [SUB leave [SL:DATE_TIME_DEPARTURE now ] ] ] ] ] ] ] ] ] ] ]\n",
            "[[(0, 13), 9], [(0, 1), 1], [(1, 13), 4], [(1, 2), 1], [(2, 13), 4], [(2, 3), 1], [(3, 13), 4], [(3, 4), 1], [(4, 13), 4], [(4, 5), 1], [(5, 13), 4], [(5, 7), 10], [(5, 6), 1], [(6, 7), 1], [(7, 13), 4], [(7, 9), 11], [(7, 8), 1], [(8, 9), 1], [(9, 13), 4], [(9, 10), 1], [(10, 13), 4], [(10, 11), 1], [(11, 13), 4], [(11, 12), 1], [(12, 13), 12]]\n",
            "\n",
            "['when', 'should', 'i', 'leave', 'my', 'house', 'to', 'get', 'to', 'the', 'hamilton', 'mall', 'right', 'when', 'it', 'opens', 'on', 'saturday']\n",
            "when should i leave my house to get to the hamilton mall right when it opens on saturday\t[IN:GET_ESTIMATED_DEPARTURE when [SUB should [SUB i [SUB leave [SUB [SL:SOURCE--IN:GET_LOCATION_HOME [SL:CONTACT my ] house ] [SUB to [SUB get [SUB to [SUB [SL:DESTINATION--IN:GET_LOCATION--SL:POINT_ON_MAP the [SUB hamilton mall ] ] [SL:DATE_TIME_ARRIVAL right [SUB when [SUB it [SUB opens [SUB on saturday ] ] ] ] ] ] ] ] ] ] ] ] ] ]\n",
            "[[(0, 18), 13], [(0, 1), 1], [(1, 18), 4], [(1, 2), 1], [(2, 18), 4], [(2, 3), 1], [(3, 18), 4], [(3, 4), 1], [(4, 18), 4], [(4, 6), 14], [(4, 5), 15], [(5, 6), 1], [(6, 18), 4], [(6, 7), 1], [(7, 18), 4], [(7, 8), 1], [(8, 18), 4], [(8, 9), 1], [(9, 18), 4], [(9, 12), 16], [(9, 10), 1], [(10, 12), 4], [(10, 11), 1], [(11, 12), 1], [(12, 18), 11], [(12, 13), 1], [(13, 18), 4], [(13, 14), 1], [(14, 18), 4], [(14, 15), 1], [(15, 18), 4], [(15, 16), 1], [(16, 18), 4], [(16, 17), 1], [(17, 18), 1]]\n",
            "\n",
            "['i', 'need', 'to', 'know', 'if', 'there', \"'s\", 'a', 'lot', 'of', 'traffic', 'on', 'my', 'way', 'home']\n",
            "i need to know if there 's a lot of traffic on my way home\t[IN:GET_INFO_TRAFFIC i [SUB need [SUB to [SUB know [SUB if [SUB there [SUB 's [SUB a [SUB lot [SUB of [SUB traffic [SUB on [SUB my [SUB way [SL:DESTINATION--IN:GET_LOCATION_HOME home ] ] ] ] ] ] ] ] ] ] ] ] ] ] ]\n",
            "[[(0, 15), 17], [(0, 1), 1], [(1, 15), 4], [(1, 2), 1], [(2, 15), 4], [(2, 3), 1], [(3, 15), 4], [(3, 4), 1], [(4, 15), 4], [(4, 5), 1], [(5, 15), 4], [(5, 6), 1], [(6, 15), 4], [(6, 7), 1], [(7, 15), 4], [(7, 8), 1], [(8, 15), 4], [(8, 9), 1], [(9, 15), 4], [(9, 10), 1], [(10, 15), 4], [(10, 11), 1], [(11, 15), 4], [(11, 12), 1], [(12, 15), 4], [(12, 13), 1], [(13, 15), 4], [(13, 14), 1], [(14, 15), 18]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j_0UKnEHN4R"
      },
      "source": [
        "### **Defining the Neural Network**\n",
        "\n",
        "#### **Sentence Encoding**\n",
        "\n",
        "We use a Bi-directional LSTM for sentence encoding. We build a sentence encoder with a embedding layer and a Bi-directional LSTM layer:\n",
        "\n",
        "- Input: \n",
        " - word indices: `[batch_size, sentence_length]`\n",
        "- Output: \n",
        "  - word embeddings: `[batch_size, sentence_length, 2*hidden_size]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTq0SpbfHN4S"
      },
      "source": [
        "class SentEnc(nn.Module):\n",
        "    def __init__(self, num_words, num_layers, hidden_size, dropout=0):\n",
        "        super(SentEnc, self).__init__()\n",
        "    \n",
        "        self.embedding = nn.Embedding(num_words, hidden_size)\n",
        "        # --------- Your code --------- #\n",
        "        # Construct your lstm module here (single line):\n",
        " \n",
        "\n",
        "        # --------- Your code ends --------- #\n",
        "    \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: [batch_size, entence_length] matrix of word indices\n",
        "        This function should return a matrix of \n",
        "        [batch_size, sentence_length, 2*hidden_size] word embeddings. \n",
        "        '''\n",
        "        # --------- Your code --------- #\n",
        "\n",
        "\n",
        "        # --------- Your code ends --------- #\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em12BpkGHN4X"
      },
      "source": [
        "### **Span Encodings**\n",
        "\n",
        "Given the LSTM outputs, we generate the span embeddings with the span indices.\n",
        "\n",
        "We generate a span embedding by concatenating the word embeddings of the first and last words of a span. For example, if a span starts from the i-th word and ends at the j-th word, our span embedding would be\n",
        "\n",
        "$$[h_i^T; h_{j-1}^T]^T$$\n",
        "\n",
        "where $h_i$ stands for the Bi-LSTM output of the $i^{th}$ word. Note that span_ij is inclusive to i but exclusive to j, as would be the output if you sliced a Python list A[i:j]. \n",
        "\n",
        "\n",
        "In Pytorch, Given the hidden states $h[0], h[1], ..., h[n]$, where\n",
        "```\n",
        "h[i].size() = [1, k]\n",
        "```\n",
        "the embedding of span (i, j) is\n",
        "```\n",
        "span_ij = torch.cat([h[i], h[j-1]], dim=1)\n",
        "span_ij.size() = [1, 2 * k]\n",
        "```\n",
        "Please complete the following function for generating span embeddings.\n",
        "\n",
        "- Input: \n",
        " - word embeddings: `[sentence_length, hidden_size]` \n",
        " - span indices: `[num_span, 2]`\n",
        "- Output: \n",
        " - span embeddings `[num_span, hidden_size * 2]`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afGE-9pZHN4Z"
      },
      "source": [
        "def get_span_embeddings(word_embeddings, span_indices):\n",
        "  '''\n",
        "  word_embeddings: [sentence_length, hidden_size] matrix of each word's' embeddings from the sentence\n",
        "  span_indices: [num_span, 2] matrix of all span indices\n",
        "  '''\n",
        "    # --------- Your code --------- #\n",
        "\n",
        "\n",
        "    # --------- Your code ends --------- #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmDK0xKfHN4g"
      },
      "source": [
        "### **Tag Prediction**\n",
        "\n",
        "We build a Classifier that puts the neural models together. The classifier takes word and span indices as inputs, and predict span labels by calculating word embeddings, span embeddings, and label logits. we will predict the tag of the spans with a linear classifier.\n",
        "\n",
        "- Inputs: \n",
        " - word indices: `[batch_size, num_words]`\n",
        "- Outputs: \n",
        " - span predictions: `[num_spans, num_labels]`\n",
        "\n",
        "Please implement the forward function following 4 steps:\n",
        "1. Generate the word embeddings by processing the input sentences with the LSTM sentence encoder.\n",
        "2. Apply dropout on word embeddings.\n",
        "3. Calculate span embeddings with function get_span_embeddings().\n",
        "4. Calculate label logits with the linear layer defined as follows.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtWAptvBHN4i"
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_words, num_labels, num_layers, hidden_size, dropout=0):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.sent_enc = SentEnc(num_words, num_layers, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(4 * hidden_size, num_labels)\n",
        "    \n",
        "    def forward(self, x, span_indices):\n",
        "      '''\n",
        "      x: [batch_size, num_words] matrix of the word indices\n",
        "      span_indices: [num_spans, 2] matrix of the span indices\n",
        "      This function should return a matrix of [num_spans, num_labels] where\n",
        "      each row contains the label logits corresponding to the relevant span.\n",
        "      You can assume batch_size is 1. \n",
        "      '''\n",
        "        # --------- Your code --------- #\n",
        "\n",
        "        \n",
        "        # --------- Your code ends --------- #\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUMXzQPS3Xh2"
      },
      "source": [
        "#For decoding, we add some random spans and label them as \"None\"\n",
        "def add_none_span(word_list, span_list, label_dict, all=False):\n",
        "    num_words = len(word_list)\n",
        "    num_labeled_span = len(span_list)\n",
        "    labeled_span_set = set([span for span, label in span_list])\n",
        "    none_spans = []\n",
        "    for i in range(num_words):\n",
        "        for j in range(i + 1, num_words):\n",
        "            if (i, j) not in labeled_span_set:\n",
        "                none_spans.append([(i, j), label_dict['None']])\n",
        "    if not all:\n",
        "        k = min(num_labeled_span, len(none_spans))\n",
        "        sampled_none_spans = random.sample(none_spans, k)\n",
        "    else:\n",
        "        sampled_none_spans = none_spans\n",
        "    return span_list + sampled_none_spans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0VYVg6RHN4o"
      },
      "source": [
        "### **Training Loop**\n",
        "\n",
        "With all neural models already defined, we can now implement the training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B82dn5UhHN4p"
      },
      "source": [
        "print('Using device: {}'.format(device))\n",
        "\n",
        "\n",
        "# just remember you can tune these hyper-parameters!\n",
        "batch_size = 1\n",
        "num_layers = 2\n",
        "hidden_size = 200\n",
        "lr = 0.05\n",
        "num_epochs = 3 # Be aware of over-fitting!\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "dropout = 0.25\n",
        "\n",
        "classifier = Classifier(num_words, num_labels, num_layers, hidden_size, dropout)\n",
        "optimizer = optim.SGD(classifier.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "classifier = classifier.to(device)\n",
        "classifier.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    classifier.train()\n",
        "    for i in tqdm(range(len(corpus_train))):\n",
        "\n",
        "        if i % 10000 == 0:\n",
        "            print('Epoch {} Batch {}'.format(epoch, i))\n",
        "        \n",
        "        cur_spans = add_none_span(corpus_train[i], spans_train[i], label_dict)\n",
        "        \n",
        "        sent_inputs  = torch.Tensor([corpus_train[i]]).long().to(device)\n",
        "        span_indices = torch.Tensor([x[0] for x in cur_spans]).long().to(device)\n",
        "        span_labels  = torch.Tensor([x[1] for x in cur_spans]).long().to(device)\n",
        "        \n",
        "          # This should follow the same training process from past homeworks. Don't\n",
        "        # forget to increment your total_loss! \n",
        "        # --------- Your code --------- #\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # --------- Your code ends --------- #\n",
        "    print('Epoch {}, train loss={}'.format(epoch, total_loss / len(corpus_train)))\n",
        "\n",
        "    total_loss = 0\n",
        "    classifier.eval()\n",
        "    for i in tqdm(range(len(corpus_valid))):\n",
        "        #if i % 10000 == 0:\n",
        "        #    print('Epoch {} Batch {}'.format(epoch, i))\n",
        "        cur_spans = add_none_span(corpus_valid[i], spans_valid[i], label_dict)\n",
        "        \n",
        "        sent_inputs  = torch.Tensor([corpus_valid[i]]).long().to(device)\n",
        "        span_indices = torch.Tensor([x[0] for x in cur_spans]).long().to(device)\n",
        "        span_labels  = torch.Tensor([x[1] for x in cur_spans]).long().to(device)\n",
        "        \n",
        "        # This is very similar to the training steps above, but you won't need to\n",
        "        # back-propogate and update weights during validation.\n",
        "        # --------- Your code --------- #\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # --------- Your code ends --------- #\n",
        "    print('Epoch {}, valid loss={}'.format(epoch, total_loss / len(corpus_valid)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4ODDfXKHN4s"
      },
      "source": [
        "### **Evaluation**\n",
        "\n",
        "After training the model, we evaluate the classification results.  \n",
        "What we will do is that we treat a tree strcture as a bag of spans (a list of span indices), and then compute F-1 score.  \n",
        "The staff solution computed precision near 0.85, recall near 0.94, f1 near 0.89, and exact_match near 0.42."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH5bRW6UHN4t"
      },
      "source": [
        "from itertools import zip_longest\n",
        "from typing import Counter, Dict, Optional\n",
        "import numpy as np\n",
        "\n",
        "class Calculator:\n",
        "    def __init__(self, strict = False) -> None:\n",
        "        self.TP = 0\n",
        "        self.gold_P = 0\n",
        "        self.pred_P = 0      \n",
        "        self.exact_match = []\n",
        "        self.tree_match = []\n",
        "        self.well_form = []\n",
        "        self.strict = strict\n",
        "\n",
        "    def get_metrics(self):\n",
        "        precision = (self.TP / self.pred_P) if self.pred_P else 0\n",
        "        recall = (self.TP / self.gold_P) if self.gold_P else 0\n",
        "        f1 = (2.0 * precision * recall / (precision + recall)) if (precision + recall) else 0\n",
        "     \n",
        "        return {\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"exact_match\": np.mean(self.exact_match),\n",
        "            \"well_form\": np.mean(self.well_form),\n",
        "            \"tree_match\":  np.mean(self.tree_match),\n",
        "            \"num_examples\": len(self.exact_match)\n",
        "        }\n",
        "    \n",
        "    def is_well_formed(self, spans):   \n",
        "        for s1 in spans: \n",
        "          for s2 in spans:\n",
        "              if s1[0] < s2[0] and s2[0] < s1[1] and s1[1] < s2[1]:\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "    def add_instance_span(self, gold_spans, pred_spans):\n",
        "        self.gold_P += len(gold_spans)\n",
        "        self.pred_P += len(pred_spans)\n",
        "        self.TP += len(set(gold_spans) & set(pred_spans))\n",
        "        self.exact_match.append(int(set(gold_spans) == set(pred_spans)))\n",
        "        gold_spans = [s[0] for s in gold_spans]\n",
        "        pred_spans = [s[0] for s in pred_spans]\n",
        "        self.tree_match.append(int(set(gold_spans) == set(pred_spans)))\n",
        "        well_formed = self.is_well_formed(pred_spans)\n",
        "        self.well_form.append(int(well_formed))\n",
        "\n",
        "    def add_instance_tree(self, gold_tree, pred_tree):\n",
        "        node_info_gold = self._get_node_info(gold_tree)\n",
        "        self.gold_P += len(node_info_gold)\n",
        "        node_info_pred = self._get_node_info(pred_tree)\n",
        "        self.pred_P += len(node_info_pred)\n",
        "        self.TP += len(node_info_gold & node_info_pred)\n",
        "        self.exact_match.append(int(node_info_gold.keys() == node_info_pred.keys()))\n",
        "        self.well_form.append(1) #we assume the decoded tree is indeed a tree :)\n",
        "        node_info_gold = {k[1] for k,v in node_info_gold.items()}\n",
        "        node_info_pred = {k[1] for k,v in node_info_pred.items()}\n",
        "        self.tree_match.append(int(node_info_gold==node_info_pred))\n",
        "        \n",
        "    def _get_node_info(self, tree) -> Counter:\n",
        "        nodes = tree.root.list_nonterminals()\n",
        "        node_info: Counter = Counter()\n",
        "        for node in nodes:\n",
        "            if node.label != 'Token':\n",
        "              span = self._get_span(node)\n",
        "              node_info[(node.label, self._get_span(node))] += 1 \n",
        "\n",
        "        return node_info\n",
        "\n",
        "    def _get_span(self, node):\n",
        "        return node.get_flat_str_spans(\n",
        "        ) if self.strict else node.get_token_span()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mamC7tgxD1r5"
      },
      "source": [
        "classifier.eval()\n",
        "parta_calc = Calculator(strict=False)\n",
        "pred_bag_spans = []\n",
        "gold_bag_spans = []\n",
        "for (tokens, spans, line) in zip(corpus_test,spans_test,test_lines):   \n",
        "    #We only test non-Token labels\n",
        "    spans = [tuple(x) for x in spans if x[1] != 1]\n",
        "\n",
        "    if len(spans) <= 1 or len(line.strip()) < 3: \n",
        "      continue\n",
        "\n",
        "    all_spans = [(i,j) for i in range(len(tokens)) \n",
        "                        for j in range(i + 1, len(tokens) + 1)]\n",
        "\n",
        "    input  = torch.Tensor([tokens]).long().to(device)\n",
        "    logits = classifier(input, torch.Tensor(all_spans).long().to(device))\n",
        "\n",
        "    pred_spans = []\n",
        "    for i, span in enumerate(all_spans):\n",
        "        label_idx = torch.argmax(logits[i]).item()\n",
        "        if label_idx != 2 and label_idx != 1:\n",
        "          pred_spans.append((span,label_idx))\n",
        "    \n",
        "    parta_calc.add_instance_span(spans, pred_spans)\n",
        "    pred_bag_spans.append(pred_spans)\n",
        "    gold_bag_spans.append(spans)\n",
        " \n",
        "print(parta_calc.get_metrics())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lasnfaK4qTfU"
      },
      "source": [
        "## **PART B** (Only for 6.864 students)\n",
        "The remaining will be **Part B** for **HW3-Trees**.  \n",
        "In Part B, we will decode a tree based on the classifier trained on Part A.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlF7r2WOylDH"
      },
      "source": [
        "### **CKY**  \n",
        "You will be implementing the following simple CYK recursion:  \n",
        "```best_score[i,j]=max_k {best_score[i,k]+best_score[k,j]} + max_l {span_dict[(i,j)][l]}```      \n",
        "where `l` is the label of the current span `(i,j)`, and `k` is the splitting point. `k` is inclusive to the right span. \n",
        "\n",
        "Note that this is a simpler recursion than the full CKY algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHPVa39Fqhhs"
      },
      "source": [
        "from torch.nn.functional import log_softmax\n",
        "EPS = 1e-6\n",
        "dp_results = []\n",
        "classifier.eval()\n",
        "for kk,(line,spans,tokens) in enumerate(zip(test_lines,spans_test,corpus_test)):\n",
        "    spans = [tuple(x) for x in spans if  x[1] != 1]\n",
        "    \n",
        "    if len(spans) <= 1 or len(line.strip()) < 3: \n",
        "      continue\n",
        "    \n",
        "    sent_inputs  = torch.Tensor([tokens]).long().to(device)\n",
        "    \n",
        "    all_spans = [(i,j) for i in range(len(tokens)) \n",
        "                         for j in range(i + 1, len(tokens) + 1)]\n",
        "    \n",
        "    logits = classifier(sent_inputs, torch.Tensor(all_spans).long().to(device))\n",
        "    logprobs = log_softmax(logits, dim = -1)\n",
        "    # span dict will map each span (l,r) to its predicted distribution of labels\n",
        "    span_dict = {}\n",
        "    for i, s in enumerate(all_spans): \n",
        "      span_dict[s]  = logprobs[i] \n",
        "  \n",
        "    TOKEN_ID, NULL_ID = 1, 2\n",
        "    best_score, best_split, best_label = {}, {}, {} # we will do dynamic programming to decode a binary tree out of our predictions\n",
        "    # Think: why do we first iterate the length of the span?\n",
        "    for ll in range(1, len(tokens) + 1): # length of the span\n",
        "        for i in range(0, len(tokens)-ll+1): # start of the span\n",
        "            j = i + ll\n",
        "            cur_span = (i, j)\n",
        "            if j == i + 1:\n",
        "                span_dict[cur_span][NULL_ID]  = -1/EPS\n",
        "                # --------- Your code --------- #\n",
        "                #use span_dict[cur_span] to update best_label and best_score              \n",
        "\n",
        "                \n",
        "                # --------- Your code ends --------- #\n",
        "                best_split[cur_span] = None\n",
        "            else:\n",
        "                span_dict[cur_span][NULL_ID]  = -1/EPS # we will never decode a NULL sub-tree\n",
        "                span_dict[cur_span][TOKEN_ID] = -1/EPS # we will never decode a NULL sub-tree\n",
        "                # --------- Your code --------- #\n",
        "                #try to give the values for best_score/label/split[cur_span] using the \n",
        "                # recursive equation above\n",
        "\n",
        "\n",
        "\n",
        "                # --------- Your code ends --------- #\n",
        "            #print(cur_span, best_score[cur_span], best_label[cur_span])\n",
        "    dp_results.append((best_score, best_split, best_label))\n",
        "print(len(dp_results))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp7gnX87QFog"
      },
      "source": [
        "### **Tree Construction**\n",
        "In this section, we will construct a tree using the DP results.  \n",
        "The code relies on a `Node` class defined in `span_tree.py`. Each `Node` object has `children`, `parent`, and `label` attributes, where `children` is a `List`, `parent` is an optional `Node`, and `label` is a `str`. \n",
        "\n",
        "You won't need more than this to fill out the code, but feel free to look more into `span_tree.py` for a better understanding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkqlsP3jW9Rc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpKRpZEiSNF1"
      },
      "source": [
        "import sys\n",
        "def get_nodetype(label):\n",
        "    if label.startswith(PREFIX_INTENT):\n",
        "        node = Intent(label)\n",
        "    elif label.startswith(PREFIX_SLOT):\n",
        "        node = Slot(label)\n",
        "    elif label.startswith(PREFIX_SUBTREE):\n",
        "        node = SubTree(label)\n",
        "    else:\n",
        "        print('something wrong with the label!!!', label)\n",
        "        sys.error()\n",
        "    return node\n",
        "\n",
        "def dfs_build(l, r, best_label, best_split):\n",
        "  '''\n",
        "  l: integer representing left (inclusive) index of span\n",
        "  r: integer representing right (non-inclusive) index of span\n",
        "  best_label: {span: label_index} dictionary created in dp_results\n",
        "  best_split: {span: split} dictionary created in dp_results\n",
        "  This function returns the node for the given span, recursively \n",
        "  creating all children nodes below it. \n",
        "  '''\n",
        "    if l + 1 == r:\n",
        "        la = best_label[(l,r)]\n",
        "        if la == 1:\n",
        "            return Token(surface_tokens[l], l)\n",
        "        else:\n",
        "            node = get_nodetype(inv_label_dict[la])\n",
        "            node.children = [Token(surface_tokens[l], l)]\n",
        "            node.children[0].parent = node\n",
        "            return node\n",
        "\n",
        "    label = inv_label_dict[best_label[(l, r)]]\n",
        "    node = get_nodetype(label)\n",
        "    \n",
        "    #--- your code --- #\n",
        "    #hint: use best_split! and recursion to assign node.children here\n",
        "\n",
        "\n",
        "    #--- your code ends --- #\n",
        "\n",
        "    for c in node.children:\n",
        "        c.parent = node\n",
        "    \n",
        "    return node\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LvUbHYcJN9G"
      },
      "source": [
        "pred_trees = []\n",
        "gold_trees = []\n",
        "partb_calc = Calculator(strict=False)\n",
        "k = 0\n",
        "for i,(line,spans,tokens) in enumerate(zip(test_lines,spans_test,corpus_test)):\n",
        "    surface_tokens, str_ref_tree = line.strip().split('\\t')\n",
        "    surface_tokens = surface_tokens.split()\n",
        "    spans = [tuple(x) for x in spans if x[1] != 1]\n",
        "\n",
        "    if len(spans) <= 1 or len(line.strip()) < 3: \n",
        "      continue\n",
        "\n",
        "    best_score, best_split, best_label = dp_results[k]\n",
        "    k+=1\n",
        "    root = Root()\n",
        "    root.children = [dfs_build(0, len(tokens), best_label, best_split)]\n",
        "    root.children[0].parent = root\n",
        "    tree = Tree('IN:GET_EVENT placeholder') #the string here is just a placeholder\n",
        "    tree.root = root\n",
        "    if k < 10: #use this info for debugging! Does your tree make sense?\n",
        "        print(k, line.strip())\n",
        "        print('REF:', str_ref_tree)\n",
        "        print('DEC:', str(tree))\n",
        "        print()\n",
        "    \"\"\" here's some decoding examples we get\n",
        "      1 whats there to do this weekend\t[IN:GET_EVENT whats [SUB there [SUB to [SUB do [SL:DATE_TIME this weekend ] ] ] ] ]\n",
        "      REF: [IN:GET_EVENT whats [SUB there [SUB to [SUB do [SL:DATE_TIME this weekend ] ] ] ] ]\n",
        "      DEC: [IN:GET_EVENT whats [SUB there [SUB to [SUB do [SL:DATE_TIME this weekend ] ] ] ] ]\n",
        "\n",
        "      2 what is a good restaurant for tex mex in austin\t[IN:UNSUPPORTED what [SUB is [SUB a [SUB good [SUB restaurant [SUB for [SUB tex [SUB mex [SUB in austin ] ] ] ] ] ] ] ] ]\n",
        "      REF: [IN:UNSUPPORTED what [SUB is [SUB a [SUB good [SUB restaurant [SUB for [SUB tex [SUB mex [SUB in austin ] ] ] ] ] ] ] ] ]\n",
        "      DEC: [IN:UNSUPPORTED what [SUB is [SUB a [SUB good [SUB restaurant [SUB for [SUB tex [SUB mex [SUB in austin ] ] ] ] ] ] ] ] ]\n",
        "\n",
        "      3 where can i see the fireworks tonight\t[IN:GET_EVENT where [SUB can [SUB i [SUB see [SUB [SL:CATEGORY_EVENT the fireworks ] [SL:DATE_TIME tonight ] ] ] ] ] ]\n",
        "      REF: [IN:GET_EVENT where [SUB can [SUB i [SUB see [SUB [SL:CATEGORY_EVENT the fireworks ] [SL:DATE_TIME tonight ] ] ] ] ] ]\n",
        "      DEC: [IN:GET_EVENT where [SUB can [SUB i [SUB see [SUB the [SUB fireworks [SL:DATE_TIME tonight ] ] ] ] ] ] ]\n",
        "    \"\"\"\n",
        "    partb_calc.add_instance_tree(Tree(str_ref_tree), tree)\n",
        "    pred_trees.append(tree)\n",
        "    gold_trees.append(Tree(str_ref_tree))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTMGdjeFV1SQ"
      },
      "source": [
        "Once again, we'll look at the metrics. \n",
        "The staff solution computed precision near 0.87, recall near 0.86, f1 near 0.87, and exact_match near 0.44. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io91TrAERMhB"
      },
      "source": [
        "print(partb_calc.get_metrics())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvT7-lGzbT2W"
      },
      "source": [
        "\n",
        "Recommended Reading (not required, just for interested students):  \n",
        "https://arxiv.org/pdf/1810.07942.pdf  \n",
        "https://www.aclweb.org/anthology/D16-1257/  \n",
        "https://arxiv.org/abs/1412.7449  "
      ]
    }
  ]
}