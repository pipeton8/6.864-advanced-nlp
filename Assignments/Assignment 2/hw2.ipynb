{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pipeton8/6.864-advanced-nlp/blob/main/Assignments/Assignment%202/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU7xWiY6TyWS"
      },
      "source": [
        "# %%bash\n",
        "# !(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n",
        "from IPython.display import clear_output\n",
        "\n",
        "!rm -rf hw2\n",
        "!wget https://github.com/pipeton8/6.864-advanced-nlp/raw/main/Assignments/Assignment%202/lab_util.py -P hw2\n",
        "!wget https://github.com/pipeton8/6.864-advanced-nlp/raw/main/Assignments/Assignment%202/reviews.csv -P hw2\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0MHaHrdUACZ"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(1,'/content/hw2')\n",
        "\n",
        "\n",
        "import csv\n",
        "import itertools as it\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import lab_util\n",
        "\n",
        "import sklearn.decomposition\n",
        "import sklearn.linear_model\n",
        "\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ3MUj4iUf76"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, you will find code scaffolding for the implementation portion of Homework 2. There are certain parts of the scaffolding marked with `# Your code here!` comments where you can fill in code to perform the specified tasks. After implementing the methods in this notebook, you will need to design and perform experiments to evaluate each method and respond to the questions in the Homework 2 handout (available on Canvas). You should be able to complete this assignment without changing any of the scaffolding code, just writing code to fill in the scaffolding and run experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG654Y9J3yHw"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We're going to be working with a dataset of product reviews. The following cell loads the dataset and splits it into training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwiX-Tc9V1xI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9821d4b-ac8e-4694-ebf8-91eec63ef3af"
      },
      "source": [
        "data = []\n",
        "n_positive = 0\n",
        "n_disp = 0\n",
        "\n",
        "with open(\"hw2/reviews.csv\") as reader:\n",
        "  csvreader = csv.reader(reader)\n",
        "  next(csvreader)\n",
        "  for id, review, label in csvreader:\n",
        "    label = int(label)\n",
        "\n",
        "    # hacky class balancing\n",
        "    if label == 1:\n",
        "      if n_positive == 2000:\n",
        "        continue\n",
        "\n",
        "      n_positive += 1\n",
        "\n",
        "    if len(data) == 4000:\n",
        "      break\n",
        "\n",
        "    data.append((review, label))\n",
        "    \n",
        "    if n_disp > 5:\n",
        "      continue\n",
        "\n",
        "    n_disp += 1\n",
        "\n",
        "    print(\"review:\", review)\n",
        "    print(\"rating:\", label, \"(good)\" if label == 1 else \"(bad)\")\n",
        "    print()\n",
        "\n",
        "print(f\"Read {len(data)} total reviews.\")\n",
        "\n",
        "# Shuffle before splitting\n",
        "np.random.shuffle(data)\n",
        "reviews, labels = zip(*data)\n",
        "\n",
        "# Training (3000 samples)\n",
        "train_reviews = reviews[:3000]\n",
        "train_labels = labels[:3000]\n",
        "\n",
        "# Validation (500 samples)\n",
        "val_reviews = reviews[3000:3500]\n",
        "val_labels = labels[3000:3500]\n",
        "\n",
        "# Testing (500 samples)\n",
        "test_reviews = reviews[3500:]\n",
        "test_labels = labels[3500:]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "review: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
            "rating: 0 (bad)\n",
            "\n",
            "review: This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
            "rating: 0 (bad)\n",
            "\n",
            "review: Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.\n",
            "rating: 1 (good)\n",
            "\n",
            "Read 4000 total reviews.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twLHWqM6Z5xD"
      },
      "source": [
        "# Part 1: word representations via matrix factorization\n",
        "\n",
        "First, we'll construct the term-document matrix (look at `/content/hw2/lab_util.py` in the file browser on the left if you want to see how this works)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WPt6Y7-Z_7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c48f07c3-5397-4962-9f9e-ee2f171b8c3a"
      },
      "source": [
        "vectorizer = lab_util.CountVectorizer()\n",
        "vectorizer.fit(train_reviews)\n",
        "td_matrix = vectorizer.transform(train_reviews).T\n",
        "print(f\"TD matrix is {td_matrix.shape[0]} x {td_matrix.shape[1]}\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TD matrix is 2062 x 3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd3-pw4XbD4B"
      },
      "source": [
        "First, implement the function `learn_reps_lsa` that computes word representations via latent semantic analysis. The `sklearn.decomposition` or `np.linalg` packages may be useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KASVs8KubeBE"
      },
      "source": [
        "def idf_matrix(matrix, idf_weight = 'idf'):\n",
        "  \"\"\" Given a |V| x n TD matrix, returns the |V| IDF vector associated, where\n",
        "      idf(w) = log (# documents / # documents that contain w)\n",
        "  \"\"\"\n",
        "  reviews_per_word = np.sum(matrix > 0, axis=1, keepdims=True)\n",
        "  reviews = matrix.shape[1]\n",
        "\n",
        "  if idf_weight == 'idf':\n",
        "    return np.log(reviews/(1+reviews_per_word))\n",
        "\n",
        "def tf_matrix(matrix, tf_weight = 'raw'):\n",
        "  \"\"\"\n",
        "    Given a |V| x n TD matrix, returns the |V| x n tf matrix, weighted according\n",
        "    to tf_weight.\n",
        "  \"\"\"\n",
        "\n",
        "  if tf_weight == 'raw':\n",
        "    tf_matrix = matrix\n",
        "\n",
        "  elif tf_weight == 'term frequency':\n",
        "    tf_matrix = matrix/matrix.sum()\n",
        "\n",
        "  elif tf_weight == 'log normalization':\n",
        "    tf_matrix = np.log(1+matrix)\n",
        "\n",
        "  elif tf_weight.find('double') != -1:\n",
        "    try:\n",
        "      k = int(tf_weight[-1])\n",
        "      tf_matrix =  k + (1-k) * matrix/matrix.max(axis=0,keepdims=True)\n",
        "    except:\n",
        "      print(\"Double-k misspecified, raw weight applied\")\n",
        "      tf_matrix = matrix\n",
        "\n",
        "  return tf_matrix\n",
        "\n",
        "def learn_reps_lsa(matrix, rep_size, tf_weight='raw', tf_idf=False, idf_weight='idf'):\n",
        "    # `matrix` is a `|V| x n` matrix (usually a TD matrix), \n",
        "    # where `|V|` is the number of words in the vocabulary and `n`\n",
        "    # is the number of reviews in the (training) corpus.\n",
        "    # This function should return a `|V| x rep_size` matrix with each\n",
        "    # row corresponding to a word representation.\n",
        "    \n",
        "    LSA_matrix = tf_matrix(matrix,tf_weight)\n",
        "\n",
        "    if tf_idf:\n",
        "      # Get IDF vector\n",
        "      idf = idf_matrix(matrix, idf_weight)\n",
        "\n",
        "      # Compute TF-IDF matrix\n",
        "      LSA_matrix = LSA_matrix * idf\n",
        "\n",
        "    # Initialize svd object\n",
        "    svd = sklearn.decomposition.TruncatedSVD(n_components=rep_size)\n",
        "\n",
        "    # Decompose TF-IDF matrix\n",
        "    return svd.fit_transform(LSA_matrix)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKWzRC0dclVK"
      },
      "source": [
        "Let's look at some representations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmRFpxYx61BM"
      },
      "source": [
        "words = [\"good\", \"bad\", \"cookie\", \"jelly\", \"dog\", \"the\", \"3\"]\n",
        "show_tokens = [vectorizer.tokenizer.word_to_token[word] for word in words]"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQJyABwM5RwN"
      },
      "source": [
        "reps = learn_reps_lsa(td_matrix, 500)\n",
        "lab_util.show_similar_words(vectorizer.tokenizer, reps, show_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsOAGLB3iRjT"
      },
      "source": [
        "We've been operating on the raw count matrix, but in class we discussed several reweighting schemes aimed at making LSA representations more informative. \n",
        "\n",
        "Here, implement the TF-IDF transform and see how it affects learned representations. While it is okay (and in fact encouraged) to use vectorized numpy operations, you should refrain from using pre-implemented library functions for computing TF-IDF.\n",
        "\n",
        "How does TF-IDF normalization change the learned similarity function?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV5xKLYTi7LA"
      },
      "source": [
        "reps_tfidf = learn_reps_lsa(td_matrix, 500, tf_idf=True)\n",
        "lab_util.show_similar_words(vectorizer.tokenizer, reps_tfidf, show_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO-NG4u1kG9z"
      },
      "source": [
        "Now that we have some representations, let's see if we can do something useful with them.\n",
        "\n",
        "Below, implement a feature function that represents a document as the sum of its\n",
        "learned word embeddings.\n",
        "\n",
        "The remaining code trains a logistic regression model on a set of *labeled* reviews; we're interested in seeing how much representations learned from *unlabeled* reviews improve classification.\n",
        "\n",
        "(Note: the staff solutions for each of the three featurizers achieve accuracies of between .78 and .83 with the full training corpus (3000 examples).)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B08xvIFlee3"
      },
      "source": [
        "def word_featurizer(xs):\n",
        "    # normalize\n",
        "    return xs / np.sqrt((xs ** 2).sum(axis=1, keepdims=True))\n",
        "\n",
        "def lsa_featurizer(xs, reps):\n",
        "    # This function takes in a `|D| x |V|` TD matrix in which each row contains\n",
        "    # the word counts for the given review.\n",
        "    # It should return a matrix where each row contains the learned feature\n",
        "    # representation of each review (e.g. the sum of LSA word representations).\n",
        "    # (Hint: use TF-IDF LSA features, which should be a global variable after\n",
        "    # running the previous cell; no need to pass it in as an argument.)\n",
        "    feats = np.zeros((xs.shape[0],reps.shape[1]))\n",
        "    idf = idf_matrix(xs.T, 'idf')\n",
        "\n",
        "    for i in range(xs.shape[0]):\n",
        "      doc_wordCount = (xs[i,:] > 0).T.reshape((-1,1))\n",
        "      feats[i,:] = np.sum(doc_wordCount * idf * reps,axis=0, keepdims=True)\n",
        "\n",
        "    # normalize\n",
        "    return feats / np.sqrt((feats ** 2).sum(axis=1, keepdims=True))\n",
        "\n",
        "# We've implemented the remainder of the training and evaluation pipeline,\n",
        "# so you likely won't need to modify the following four functions.\n",
        "def combo_featurizer(xs, reps):\n",
        "    return np.concatenate((word_featurizer(xs), lsa_featurizer(xs, reps)), axis=1)\n",
        "\n",
        "def train_model(xs, ys, featurizer, reps = None):\n",
        "    if reps is None:\n",
        "      xs_featurized = featurizer(xs)\n",
        "    else:\n",
        "      xs_featurized = featurizer(xs, reps)\n",
        "\n",
        "    model = sklearn.linear_model.LogisticRegression()\n",
        "    model.fit(xs_featurized, ys)\n",
        "\n",
        "    return model\n",
        "\n",
        "def eval_model(model, xs, ys, featurizer, reps=None):\n",
        "    if reps is None:\n",
        "      xs_featurized = featurizer(xs)\n",
        "    else:\n",
        "      xs_featurized = featurizer(xs, reps)\n",
        "\n",
        "    pred_ys = model.predict(xs_featurized)\n",
        "\n",
        "    return np.mean(pred_ys == ys)\n",
        "\n",
        "def training_experiment(name, n_train, featurizer, reps=None, verbose=True):\n",
        "    if verbose:\n",
        "      print(f\"{name} features, {n_train} examples\")\n",
        "\n",
        "    train_xs = vectorizer.transform(train_reviews[:n_train])\n",
        "    train_ys = train_labels[:n_train]\n",
        "    test_xs = vectorizer.transform(test_reviews)\n",
        "    test_ys = test_labels\n",
        "    model = train_model(train_xs, train_ys, featurizer, reps)\n",
        "    acc = eval_model(model, test_xs, test_ys, featurizer, reps)\n",
        "\n",
        "    if verbose:\n",
        "      print(f\"Accuracy is {acc*100:.2f}%\")\n",
        "      print(\"\")\n",
        "\n",
        "    return acc"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cuhuj7Ek1Q8i",
        "outputId": "25a6ceea-b900-4b4f-f3f4-bf9593d21d02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Train\n",
        "n_train = 3000\n",
        "embed_size = 500\n",
        "\n",
        "reps_tfidf = learn_reps_lsa(td_matrix, embed_size, tf_idf=True)\n",
        "acc = training_experiment(\"word\", n_train, word_featurizer)\n",
        "acc = training_experiment(\"lsa\", n_train, lsa_featurizer, reps=reps_tfidf)\n",
        "acc = training_experiment(\"combo\", n_train, combo_featurizer, reps=reps_tfidf)\n",
        "print()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word features, 3000 examples\n",
            "Accuracy is 78.00%\n",
            "\n",
            "lsa features, 3000 examples\n",
            "Accuracy is 81.20%\n",
            "\n",
            "combo features, 3000 examples\n",
            "Accuracy is 80.00%\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpXziVNrlfp2"
      },
      "source": [
        "**Part 1: Lab writeup**\n",
        "\n",
        "Part 1 of your lab report should discuss any implementation details that were important to filling out the code above, as well as your answers to the questions in Part 1 of the Homework 2 handout. Below, you can set up and perform experiments that answer these questions (include figures, plots, and tables in your write-up as you see fit)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3WrzsHhC1I5"
      },
      "source": [
        "## Experiments for Part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTVF9JdRNOsx"
      },
      "source": [
        "# The following four lines will run a training experiment with all 3k examples\n",
        "# in training set for each feature type. `training_experiment` may be useful to\n",
        "# you when performing experiments to answer questions in the handout.\n",
        "n_train = 3000\n",
        "embed_sizes = [100*i for i in range(1,11)]\n",
        "accs = {'word'  : [],\n",
        "        'lsa'   : [],\n",
        "        'combo' : []\n",
        "        }\n",
        "\n",
        "features = ['word', 'lsa', 'combo']\n",
        "featurizers = [word_featurizer, lsa_featurizer, combo_featurizer]\n",
        "\n",
        "for embed_size in tqdm(embed_sizes):\n",
        "  reps = learn_reps_lsa(td_matrix, embed_size, tf_weight='term frequency', tf_idf=True)\n",
        "  # print(f\"Using embedding size of {embed_size} dimensions.\")\n",
        "  # print(\"-\"*40)\n",
        "\n",
        "  for feature,featurizer in zip(features,featurizers):\n",
        "    if feature == 'word':\n",
        "      accs[feature].append(training_experiment(feature, n_train, featurizer, verbose =False))\n",
        "    else:\n",
        "      accs[feature].append(training_experiment(feature, n_train, featurizer,reps, verbose=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKZlhNOUsGu1"
      },
      "source": [
        "print(\"Max accuracies:\")\n",
        "for feature in features:\n",
        "  print(\" \"*3 + f\"{feature}: {sorted(accs[feature])[-1]*100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNcvPN4tmdOl"
      },
      "source": [
        "offset = (0,0.5)\n",
        "fig, ax = plt.subplots()\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "\n",
        "for feature in features:\n",
        "  x = embed_sizes\n",
        "  y = [acc*100 for acc in accs[feature]]\n",
        "  sns.lineplot(x=x, y=y, label=feature, marker='o', ax=ax)\n",
        "\n",
        "  if feature == 'combo':\n",
        "    for x_i,y_i in zip(x,y):\n",
        "      s = f'{y_i:.1f}%'\n",
        "      ax.text(x=x_i+offset[0], \n",
        "              y=y_i+offset[1],\n",
        "              rotation=45,\n",
        "              fontsize=7,\n",
        "              s=s)\n",
        "\n",
        "plt.yticks([75 + 2*x for x in range(6)])\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xticks(embed_sizes)\n",
        "plt.xlabel(\"Embedding size (dimensions)\")\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxfunCYh5nmZ"
      },
      "source": [
        "# Part 2: word representations via language modeling\n",
        "\n",
        "In this section, we'll train a word embedding model with a word2vec-style objective rather than a matrix factorization objective. This requires a little more work; we've provided scaffolding for a PyTorch model implementation below.\n",
        "If you don't have much PyTorch experience, there are some tutorials [here](https://pytorch.org/tutorials/) which may be useful. You're also welcome to implement these experiments in any other framework of your choosing (note that we won't be able to provide debugging support if you use a different framework)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1napibQ6aub"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as torch_data\n",
        "\n",
        "class Word2VecModel(nn.Module):\n",
        "    # A torch module implementing a word2vec predictor. The `forward` function\n",
        "    # should take a batch of context word ids as input and predict the word \n",
        "    # in the middle of the context as output, as in the CBOW model from lecture.\n",
        "    # Hint: look at how padding is handled in lab_util.get_ngrams when\n",
        "    # initializing `ctx`: vocab_size is used as the padding token for contexts\n",
        "    # near the beginning and end of sequences. If you use an embedding module\n",
        "    # in your Word2Vec implementation, make sure to account for this extra\n",
        "    # padding token, and account for it with the `padding_idx` kwarg.\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, padding_idx=2006):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.padding_idx = padding_idx\n",
        "        self.input_embed = nn.Embedding(vocab_size+1,embed_dim,padding_idx=padding_idx)\n",
        "        self.output_embed = nn.Embedding(vocab_size+1,embed_dim,padding_idx=padding_idx)\n",
        "\n",
        "    def forward(self, context):\n",
        "        # Context is an `n_batch x n_context` matrix of integer word ids\n",
        "        # this function should return an `n_batch x vocab_size` matrix with\n",
        "        # element i, j being the (possibly log) probability of the middle word\n",
        "        # in context i being word j.\n",
        "        \n",
        "        # Embed context using output embedding matrix\n",
        "        embed_context = self.output_embed(context)\n",
        "\n",
        "        # Project embedded context using average\n",
        "        proj_context = torch.mean(embed_context, 1) # n_batch x embed_size\n",
        "\n",
        "        # Obtain probability\n",
        "        prob = torch.matmul(proj_context,self.input_embed.weight.t())[:,:-1]\n",
        "\n",
        "        # Return log prob\n",
        "        return F.log_softmax(prob,dim=1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePgZlityuWr3"
      },
      "source": [
        "def learn_reps_word2vec(corpus, window_size, rep_size, n_epochs, n_batch, device='cuda', verbose=True):\n",
        "    # This method takes in a corpus of training sentences. It returns a matrix\n",
        "    # of word embeddings with the same structure as used in the previous section \n",
        "    # of the assignment. (You can extract this matrix from the parameters of the \n",
        "    # Word2VecModel.)\n",
        "\n",
        "    tokenizer = lab_util.Tokenizer()\n",
        "    tokenizer.fit(corpus)\n",
        "    tokenized_corpus = tokenizer.tokenize(corpus)\n",
        "\n",
        "    ngrams = lab_util.get_ngrams(tokenized_corpus, window_size, pad_idx=2006)\n",
        "\n",
        "    model = Word2VecModel(tokenizer.vocab_size, rep_size).to(device)\n",
        "    opt = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    loader = torch_data.DataLoader(ngrams, batch_size=n_batch, shuffle=True)\n",
        "\n",
        "    # Use the Negative Log Likelihood loss\n",
        "    loss_fn = nn.NLLLoss(reduction='sum')\n",
        "\n",
        "    losses = []  # Potentially useful for debugging (loss should go down!)\n",
        "    for epoch in tqdm(range(n_epochs)):\n",
        "        epoch_loss = 0\n",
        "        for context, label in loader:\n",
        "          context = context.to(device)\n",
        "          label = label.to(device)\n",
        "\n",
        "          # As described above, `context` is a batch of context word ids, and\n",
        "          # `label` is a batch of predicted word labels.\n",
        "\n",
        "          # Obtain predictions (log-probabilities) using forward of model\n",
        "          preds = model(context)\n",
        "\n",
        "          # Now finish the backward pass and gradient update.\n",
        "          # Remember, you need to compute the loss, zero the gradients\n",
        "          # of the model parameters, perform the backward pass, and\n",
        "          # update the model parameters.\n",
        "          loss = loss_fn(preds,label)\n",
        "          loss.backward()          \n",
        "          opt.step()\n",
        "          opt.zero_grad()\n",
        "\n",
        "          epoch_loss += loss.item()\n",
        "\n",
        "        \n",
        "        if verbose:\n",
        "          print(f\"Loss in epoch {epoch+1}\" + \" \"*(epoch<n_epochs-1) + f\": {epoch_loss:.2f}\")\n",
        "\n",
        "        losses.append(epoch_loss)\n",
        "\n",
        "    # Hint: you want to return a `vocab_size x embedding_size` numpy array\n",
        "    # Return embedding matrix\n",
        "    input_embedding_matrix = model.input_embed.weight.data[:-1,:].cpu().numpy()\n",
        "    output_embedding_matrix = model.output_embed.weight.data[:-1,:].cpu().numpy()\n",
        "\n",
        "    embedding_matrix = 1/2 * (input_embedding_matrix + output_embedding_matrix)\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaUy1cNuB3W1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "4a1acadd-b6aa-41ad-a800-f00f4664161a"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# Parameters\n",
        "window_size = 2\n",
        "rep_size    = 500\n",
        "n_epochs    = 20\n",
        "n_batch     = 100\n",
        "device      = 'cuda'\n",
        "\n",
        "# Representations\n",
        "reps_word2vec = learn_reps_word2vec(train_reviews, window_size, rep_size, n_epochs, n_batch, device=device)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2062\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d35b91fc0ae464aa7729fcc938ac54c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss in epoch 1 : 5461337.02\n",
            "Loss in epoch 2 : 2821589.28\n",
            "Loss in epoch 3 : 2055930.28\n",
            "Loss in epoch 4 : 1694567.31\n",
            "Loss in epoch 5 : 1483564.26\n",
            "Loss in epoch 6 : 1344330.07\n",
            "Loss in epoch 7 : 1245793.96\n",
            "Loss in epoch 8 : 1172157.15\n",
            "Loss in epoch 9 : 1115703.59\n",
            "Loss in epoch 10 : 1071221.79\n",
            "Loss in epoch 11 : 1035082.91\n",
            "Loss in epoch 12 : 1006018.05\n",
            "Loss in epoch 13 : 982223.35\n",
            "Loss in epoch 14 : 961900.13\n",
            "Loss in epoch 15 : 945524.66\n",
            "Loss in epoch 16 : 931833.08\n",
            "Loss in epoch 17 : 920135.14\n",
            "Loss in epoch 18 : 909971.49\n",
            "Loss in epoch 19 : 901514.96\n",
            "Loss in epoch 20: 894359.48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3oE-tpR7I39"
      },
      "source": [
        "After training the embeddings, we can try to visualize the embedding space to see if it makes sense. First, we can take any word in the space and check its closest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMW4QND56bHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29ec95cc-c8ad-443f-8b33-3c61252e127c"
      },
      "source": [
        "lab_util.show_similar_words(vectorizer.tokenizer, reps_word2vec, show_tokens)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good 15\n",
            "  sweetner 1.665\n",
            "  with 1.712\n",
            "  is 1.713\n",
            "  sound 1.725\n",
            "  popcorn 1.726\n",
            "bad 55\n",
            "  isn't 1.701\n",
            "  left 1.720\n",
            "  went 1.722\n",
            "  purchase 1.732\n",
            "  review 1.733\n",
            "cookie 1147\n",
            "  recipes 1.636\n",
            "  recipe 1.668\n",
            "  crumbles 1.673\n",
            "  sandwich 1.688\n",
            "  bars 1.689\n",
            "jelly 1162\n",
            "  licorice 1.558\n",
            "  beans 1.682\n",
            "  cheap 1.695\n",
            "  apple 1.717\n",
            "  10 1.721\n",
            "dog 279\n",
            "  actual 1.665\n",
            "  bought 1.697\n",
            "  he 1.698\n",
            "  likes 1.706\n",
            "  adult 1.711\n",
            "the 34\n",
            "  by 1.561\n",
            "  in 1.652\n",
            "  at 1.674\n",
            "  but 1.700\n",
            "  try 1.702\n",
            "3 390\n",
            "  2 1.621\n",
            "  1 1.661\n",
            "  4 1.681\n",
            "  tablespoons 1.686\n",
            "  along 1.688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue-9CPSc7fi9"
      },
      "source": [
        "We can also cluster the embedding space. Clustering in 4 or more dimensions is hard to visualize, and even clustering in 2 or 3 can be difficult because there are so many words in the vocabulary. One thing we can try to do is assign cluster labels and qualitiatively look for an underlying pattern in the clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-Yf6NMCXVx4"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "indices = KMeans(n_clusters=10).fit_predict(reps_word2vec)\n",
        "zipped = list(zip(range(vectorizer.tokenizer.vocab_size), indices))\n",
        "np.random.shuffle(zipped)\n",
        "zipped = zipped[:100]\n",
        "zipped = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
        "for token, cluster_idx in zipped:\n",
        "    word = vectorizer.tokenizer.token_to_word[token]\n",
        "    print(f\"{word}: {cluster_idx}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci1TkENU78Wn"
      },
      "source": [
        "Finally, we can use the trained word embeddings to construct vector representations of full reviews. One common approach is to simply average all the word embeddings in the review to create an overall embedding. Implement the transform function in Word2VecFeaturizer to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5vjmRV6Dgbu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9f7da03-a7e4-40af-f6c9-faac18c89fe8"
      },
      "source": [
        "def w2v_featurizer(xs, reps):\n",
        "    # This function takes in a matrix in which each row contains the word counts\n",
        "    # for the given review. It should return a matrix in which each row contains\n",
        "    # the average Word2Vec embedding of each review (hint: this will be very\n",
        "    # similar to `lsa_featurizer` from above, just using Word2Vec embeddings \n",
        "    # instead of LSA).\n",
        "\n",
        "    feats = np.zeros((xs.shape[0],reps.shape[1]))\n",
        "    idf = idf_matrix(xs.T, 'idf')\n",
        "\n",
        "    for i in range(xs.shape[0]):\n",
        "      doc_wordCount = (xs[i,:] > 0).T.reshape((-1,1))\n",
        "      feats[i,:] = np.sum(doc_wordCount * idf * reps,axis=0, keepdims=True)\n",
        "\n",
        "    # normalize\n",
        "    return feats / np.sqrt((feats ** 2).sum(axis=1, keepdims=True))\n",
        "\n",
        "training_experiment('word2vec', 3000, w2v_featurizer, reps=reps_word2vec, verbose=True)\n",
        "print()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word2vec features, 3000 examples\n",
            "Accuracy is 78.80%\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSfoQbxaXtfH"
      },
      "source": [
        "**Part 2: Lab writeup**\n",
        "\n",
        "Part 2 of your lab report should discuss any implementation details that were important to filling out the code above, as well as your answers to the questions in Part 2 of the Homework 2 handout. Below, you can set up and perform experiments that answer these questions (include figures, plots, and tables in your write-up as you see fit)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Beb7dgqWycYx"
      },
      "source": [
        "## Experiments for Part 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obr-zEmNydDm"
      },
      "source": [
        "# Your code here!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_HYOLQd5JLx"
      },
      "source": [
        "# Part 3 (6.864 only)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70uuuHpr5NT4"
      },
      "source": [
        "In Part 3, you will extend the methods you've implemented in Parts 1 and 2 with the goal of improving final predictive performance. You should experiment with at least one idea to improve the model --- feel free to focus on either the featurizer or the classifier. Some suggestions of things you could try:\n",
        "\n",
        "1. Implement a different TD matrix normalization method (see lecture slides for alternatives to TF-IDF).\n",
        "2. Implement a different Word2Vec formulation (in Part 2, you implemented the CBOW formulation; does the skip-gram formulation perform any better?).\n",
        "3. Implement a more sophisticated classifier module.\n",
        "4. Tune featurizer and/or classifier hyperparameters (for full marks, you should obtain at least a 1% improvement in prediction accuracy if you only tune hyperparameters).\n",
        "\n",
        "In your report, discuss what you implemented (including relevant design decisions), and how your change(s) impacted performance.\n",
        "\n",
        "Note: As long as you try something with difficulty comparable to the suggested modifications and have a meaningful discussion of your results in your report, you can earn full marks (you do not necessarily need to improve performance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTCrIkai-bkF"
      },
      "source": [
        "# Your code here!"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}